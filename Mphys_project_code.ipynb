{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted N-Body Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy import constants as const\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import emcee\n",
    "import corner\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "\n",
    "%matplotlib qt\n",
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "M = 5e12 * const.M_sun.value\n",
    "SOFTENING = 5e3 * const.pc.value  # Small value to prevent division by zero\n",
    "HUBBLE_CONST = 68.17975541996692/(const.pc.value*1e3) #Physical value for t = 13.7 and Omega_Lambda = 0.69\n",
    "\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self): #initialize \n",
    "        r_0 = 0.04e6 * const.pc.value\n",
    "        phi = np.random.uniform(0,2*np.pi)\n",
    "        u = np.random.uniform(0,1)\n",
    "        theta = np.arccos(1-2*u)\n",
    "        trasnform = [np.sin(theta)* np.cos(phi) ,np.sin(theta)*np.sin(phi),np.cos(theta)]\n",
    "        self.r = np.array([r_0*a for a in trasnform])\n",
    "        E_min = -2.66e10 \n",
    "        E_max = -8e9\n",
    "        E_0 = np.random.uniform(E_min,E_max) \n",
    "        self.E_0 = E_0\n",
    "        v_tot_init = np.sqrt(2 * (E_0 + const.G.value * M / np.linalg.norm(self.r)))\n",
    "        self.v = np.array([v_tot_init*a for a in trasnform])\n",
    "        \n",
    "    def separation_calc(self):\n",
    "        return np.sqrt(self.r[0]**2+self.r[1]**2+self.r[2]**2+SOFTENING**2)\n",
    "    \n",
    "    def acc_calc(self, H_0, Omega):\n",
    "        separation = self.separation_calc()\n",
    "        return -const.G.value*M*self.r/(separation**3) + H_0**2*Omega*self.r\n",
    "\n",
    "    def update_brute(self, max_timestep,Omega_lambda):\n",
    "        dt = min(1e-6*max(np.linalg.norm(self.r), SOFTENING)/np.linalg.norm(self.v),max_timestep) #used to be 1e-5, in Jorge's 1e-4\n",
    "        acc = self.acc_calc(HUBBLE_CONST,Omega_lambda)\n",
    "        self.v += acc * dt  # Update velocity\n",
    "        self.r += self.v * dt  # Update position\n",
    "        #print(self.r /(const.pc.value*10**6))\n",
    "        #print(self.v)\n",
    "        return dt\n",
    "    \n",
    "    def total_E(self,H_0,Omega):\n",
    "        separation = self.separation_calc()\n",
    "        return np.linalg.norm(self.v)**2/2 - const.G.value*M/separation -(H_0**2*Omega*separation**2)/2\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Conversions\n",
    "\n",
    "def s_to_Gyr(t):\n",
    "    return t/(1e9*365.25*24*3600)\n",
    "def Gyr_to_s(t):\n",
    "    return t*(1e9*365.25*24*3600)\n",
    "def m_to_Mpc(d):\n",
    "    return d/(const.pc.value*10**6)\n",
    "def Mpc_to_m(d):\n",
    "    return d*(const.pc.value*10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(max_timestep,Omega_lambda):\n",
    "    particle = Particle() #Initialize\n",
    "    t=0 \n",
    "    xs = []\n",
    "    ys = []\n",
    "    zs = []\n",
    "    distances_list = []\n",
    "    vxs = []\n",
    "    vys = []\n",
    "    vzs = []\n",
    "    v_tot_list = []\n",
    "    ts = []\n",
    "    Es = []\n",
    "    save_counter = 1\n",
    "    t_tot = Gyr_to_s(13.7)\n",
    "    t_save = Gyr_to_s(0.1)\n",
    "    while t < t_tot: \n",
    "        t += particle.update_brute(max_timestep,Omega_lambda)\n",
    "        #print(t)\n",
    "        if t > t_save * save_counter: \n",
    "            xs.append(particle.r[0])\n",
    "            ys.append(particle.r[1])\n",
    "            zs.append(particle.r[2])\n",
    "            distances_list.append(particle.separation_calc())\n",
    "            vxs.append(particle.v[0])\n",
    "            vys.append(particle.v[1])\n",
    "            vzs.append(particle.v[2])\n",
    "            ts.append(t)\n",
    "            v_tot_list.append(np.linalg.norm(particle.v))\n",
    "            Es.append(particle.total_E(HUBBLE_CONST,0.69))\n",
    "            save_counter +=1\n",
    "\n",
    "    #Save last values\n",
    "    xs.append(particle.r[0])\n",
    "    ys.append(particle.r[1])\n",
    "    zs.append(particle.r[2])\n",
    "    distances_list.append(particle.separation_calc())\n",
    "    vxs.append(particle.v[0])\n",
    "    vys.append(particle.v[1])\n",
    "    vzs.append(particle.v[2])\n",
    "    ts.append(t)\n",
    "    v_tot_list.append(np.linalg.norm(particle.v))\n",
    "    Es.append(particle.total_E(HUBBLE_CONST,0.69))\n",
    "    return xs,ys,zs,distances_list,vxs,vys,vzs,v_tot_list,Es,particle.E_0\n",
    "#xs,ys,zs,distances_list,vxs,vys,vzs,v_tot_list,Es,E_0 = run(Gyr_to_s(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def run_simulation(max_timestep,n_particles,Omega_lambda):\n",
    "    df = pd.DataFrame()\n",
    "    for i in tqdm(range(n_particles)):\n",
    "        xs,ys,zs,distances_list,vxs,vys,vzs,v_tot_list,Es,E_0_comp = run(max_timestep,Omega_lambda)\n",
    "        d = {'x': xs, 'y': ys, 'z': zs, 'distances': distances_list, 'vx': vxs, 'vy': vys, 'vz': vzs, 'velocity':v_tot_list, 'Energy': Es, 'Initial energy': E_0_comp}\n",
    "        df_new = pd.DataFrame(data= d)\n",
    "        df = pd.concat([df,df_new])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1particle_test = run_simulation(Gyr_to_s(0.00001),1,0.69) # Used to be 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = pd.read_csv(\"simulation_data_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def filter_data(df):\n",
    "    # Group by blocks of 138 rows using integer division\n",
    "    grouped = [df.iloc[i:i+138] for i in range(0, len(df), 138)]\n",
    "    \n",
    "    # Keep only complete groups\n",
    "    valid_particles = [g for g in grouped if len(g) == 138]\n",
    "    \n",
    "    return valid_particles\n",
    "\n",
    "def velocity_comp(valid_particles):\n",
    "    updated_particles = []\n",
    "    \n",
    "    for particle in tqdm(valid_particles):\n",
    "        dot_product = ((particle[\"vx\"] * particle[\"x\"] + \n",
    "                        particle[\"vy\"] * particle[\"y\"] +\n",
    "                        particle[\"vz\"] * particle[\"z\"]) / particle[\"distances\"])\n",
    "        \n",
    "        signs = dot_product / np.abs(dot_product)\n",
    "        \n",
    "        # Apply changes directly to the DataFrame\n",
    "        particle = particle.copy()  # Ensure we modify a new DataFrame\n",
    "        particle[\"velocity\"] *= signs  # Modify velocity\n",
    "        particle.reset_index(inplace=True)  # Reset index\n",
    "        \n",
    "        updated_particles.append(particle)  # Store the modified DataFrame\n",
    "    \n",
    "    return updated_particles\n",
    "\n",
    "# Process the data\n",
    "valid_particles = filter_data(sim_df)\n",
    "valid_particles= velocity_comp(valid_particles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Conservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum energy ratio difference considering every point\n",
    "valid_particles_copy = valid_particles.copy()\n",
    "maximum = 0 \n",
    "for idx, particle in enumerate(valid_particles_copy):\n",
    "    particle[\"Energy ratio\"] = particle[\"Energy\"] / particle[\"Energy\"][0]\n",
    "    particle[\"Energy ratio difference\"] = np.abs(particle[\"Energy ratio\"] - 1)\n",
    "    if particle[\"Energy ratio difference\"].max() > maximum:\n",
    "        maximum = particle[\"Energy ratio difference\"].max()\n",
    "        max_particle = idx\n",
    "print(maximum, max_particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot energy conservation for a particle in  valid_particles\n",
    "energy_1 = valid_particles_copy[87][\"Energy\"]\n",
    "plt.plot(energy_1/energy_1[0])\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Energy/Initial Energy\")\n",
    "plt.title(\"Energy over time for Particle 87\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Energy_over_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distance for the same particle\n",
    "distance_1 = valid_particles_copy[87][\"distances\"]\n",
    "plt.plot(m_to_Mpc(distance_1))\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Distance[Mpc]\")\n",
    "plt.title(\"Distance to the perturber over time for a Particle\")\n",
    "#plt.savefig(\"Distance_over_time.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the distribution of energy looses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum energy loss for each particle\n",
    "\n",
    "max_energy_loss = []\n",
    "for particle in valid_particles_copy:\n",
    "    max_energy_loss.append(particle[\"Energy ratio difference\"].max()*100)\n",
    "\n",
    "# Plot the distribution of maximum energy loss\n",
    "plt.hist(max_energy_loss, bins=20)\n",
    "plt.xlabel(\"Maximum Energy Gain [%]\")   \n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Maximum Energy Gain\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"Energy_gain_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final change\n",
    "maximum = 0 \n",
    "for idx, particle in enumerate(valid_particles):\n",
    "    initial_energy = particle[\"Energy\"].iloc[0]\n",
    "    final_energy = particle[\"Energy\"].iloc[-1]\n",
    "    energy_ratio = final_energy / initial_energy\n",
    "    energy_ratio_difference = 100*(energy_ratio - 1)\n",
    "    print(energy_ratio_difference)\n",
    "    if energy_ratio_difference < 0:\n",
    "        print(\"negative!\")\n",
    "    if energy_ratio_difference > maximum:\n",
    "        maximum = energy_ratio_difference\n",
    "        max_particle = idx\n",
    "\n",
    "# plot the energy distribution\n",
    "plt.hist(energy_ratio_difference, bins=40)\n",
    "plt.xlabel(\"Maximum Energy Gain [%]\")   \n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Maximum Energy Gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum distance that a paricle moves away in the last frame\n",
    "maximum_distance = 0\n",
    "for particle in valid_particles:\n",
    "    distance = particle[\"distances\"].iloc[-1]\n",
    "    if distance > maximum_distance:\n",
    "        maximum_distance = distance\n",
    "\n",
    "print(m_to_Mpc(maximum_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = Gyr_to_s(np.linspace(1e-7, 0.001, 10))\n",
    "#print(max_timesteps)\n",
    "# run the simulation for 3 particles for each max_timestep, and save the median energy loss to plot later\n",
    "energy_loss = []\n",
    "for timestep in tqdm(max_timesteps):\n",
    "    df_1particle_test = run_simulation(timestep, 3,0.69)\n",
    "    energy_loss_temp = []\n",
    "    for i in range(0, len(df_1particle_test), 138):  # Assuming each particle has 138 rows\n",
    "        particle = df_1particle_test.iloc[i:i+138]\n",
    "        initial_energy = particle[\"Energy\"].iloc[0]\n",
    "        final_energy = particle[\"Energy\"].iloc[-1]\n",
    "        energy_ratio = final_energy / initial_energy\n",
    "        energy_ratio_difference = 100*np.abs(energy_ratio - 1)\n",
    "        energy_loss_temp.append(energy_ratio_difference)\n",
    "    energy_loss.append(np.median(energy_loss_temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_loss_percent = energy_loss\n",
    "\n",
    "plt.plot(s_to_Gyr(max_timesteps), energy_loss_percent, \".\", label=\"Data\")\n",
    "plt.xlabel(\"Timestep [Gyr]\")\n",
    "plt.ylabel(\"Median Energy Loss at the end of the simulation [%]\")\n",
    "plt.title(\"Energy Loss vs Max Timestep Allowed\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define a quadratic function to fit\n",
    "def quadratic_func(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Fit the curve\n",
    "x_data = max_timesteps\n",
    "popt, pcov = curve_fit(quadratic_func, x_data, energy_loss_percent)\n",
    "\n",
    "# Plot the fitted curve\n",
    "x_fit = max_timesteps\n",
    "y_fit = quadratic_func(x_fit, *popt)\n",
    "plt.plot(s_to_Gyr(x_fit), y_fit, label='Fitted Quadratic Curve', color='red')\n",
    "\n",
    "plt.legend()\n",
    "# plt.savefig(\"Energy_loss_vs_timestep.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D animation of x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Set plot limits (adjust based on the range of x and y data in your entire dataset)\n",
    "all_x = pd.concat([m_to_Mpc(particle_df['x']) for particle_df in valid_particles_physical])\n",
    "all_y = pd.concat([m_to_Mpc(particle_df['y']) for particle_df in valid_particles_physical])\n",
    "ax.set_xlim(all_x.min(), all_x.max())\n",
    "ax.set_ylim(all_y.min(), all_y.max())\n",
    "\n",
    "# Initialize an empty scatter plot for updating later\n",
    "scatters = ax.scatter([], [], s=10)  # Adjust particle size with 's'\n",
    "\n",
    "# Animation function to update particle positions\n",
    "def animate(frame):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "\n",
    "    # Update the data for each particle at the current frame (timestep)\n",
    "    for particle_df in valid_particles_physical:\n",
    "        if frame < len(particle_df):  # Ensure frame is within particle data\n",
    "            x = particle_df.iloc[frame]['x']\n",
    "            y = particle_df.iloc[frame]['y']\n",
    "            x_data.append(m_to_Mpc(x))\n",
    "            y_data.append(m_to_Mpc(y))\n",
    "\n",
    "    # Plot updated positions of valid_particles\n",
    "    scatters.set_offsets(np.c_[x_data, y_data])\n",
    "    \n",
    "    # Add labels, titles, or grid if needed (optional)\n",
    "    ax.set_xlabel('X Position [Mpc]', fontsize=14)\n",
    "    ax.set_ylabel('Y Position [Mpc]', fontsize=14)\n",
    "    ax.set_title(f't = {frame/10} Gyr', fontsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set up the animation\n",
    "anim = FuncAnimation(fig, animate, frames=138, interval=100)\n",
    "\n",
    "# Show the animation\n",
    "plt.show()\n",
    "#anim.save('2D_animation.mp4',  writer='ffmpeg', fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D animation of positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(velocity):\n",
    "    return 'red' if velocity > 0 else 'blue'\n",
    "\n",
    "# Create figure and axis for 3D plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Set plot limits based on the range of x, y, and z data across all particles\n",
    "all_x = pd.concat([m_to_Mpc(particle_df['x']) for particle_df in valid_particles])\n",
    "all_y = pd.concat([m_to_Mpc(particle_df['y']) for particle_df in valid_particles])\n",
    "all_z = pd.concat([m_to_Mpc(particle_df['z']) for particle_df in valid_particles])\n",
    "ax.set_xlim(all_x.min(), all_x.max())\n",
    "ax.set_ylim(all_y.min(), all_y.max())\n",
    "ax.set_zlim(all_z.min(), all_z.max())\n",
    "\n",
    "# Animation function to update particle positions in 3D\n",
    "def animate(frame):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    z_data = []\n",
    "    colors = []\n",
    "\n",
    "    # Update the data for each particle at the current frame (timestep)\n",
    "    for particle_df in valid_particles:\n",
    "        if frame < len(particle_df):  # Ensure frame is within particle data\n",
    "            x = m_to_Mpc(particle_df.iloc[frame]['x'])\n",
    "            y = m_to_Mpc(particle_df.iloc[frame]['y'])\n",
    "            z = m_to_Mpc(particle_df.iloc[frame]['z'])\n",
    "            velocity = particle_df.iloc[frame]['velocity']\n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "            z_data.append(z)\n",
    "            colors.append(get_color(velocity))\n",
    "\n",
    "    # Clear the previous scatter plot\n",
    "    ax.cla()\n",
    "\n",
    "    # Reset the axis limits\n",
    "    ax.set_xlim(all_x.min(), all_x.max())\n",
    "    ax.set_ylim(all_y.min(), all_y.max())\n",
    "    ax.set_zlim(all_z.min(), all_z.max())\n",
    "\n",
    "    # Plot updated positions of particles in 3D, recreating the scatter plot\n",
    "    ax.scatter(x_data, y_data, z_data, c=colors, s=10)  # Use 'c' for color array\n",
    "\n",
    "    # Set labels and title (optional)\n",
    "    ax.set_xlabel('X Position[Mpc]', fontsize=14)\n",
    "    ax.set_ylabel('Y Position[Mpc]', fontsize=14)\n",
    "    ax.set_zlabel('Z Position[Mpc]', fontsize=14)\n",
    "    ax.set_title(f't = {frame/10} Gyr', fontsize=14)\n",
    "\n",
    "# Set up the animation\n",
    "anim = FuncAnimation(fig, animate, frames=138, interval=10)\n",
    "\n",
    "# Show the animation\n",
    "#plt.show()\n",
    "#save the animation\n",
    "#anim.save('3D_animation_2.gif', writer='imagemagick', fps=10)\n",
    "anim.save('3D_animation.mp4', writer='ffmpeg', fps=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_3d_position_snapshots(valid_particles, timesteps=[40, 85, 130], figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Create side-by-side 3D position visualizations\n",
    "    \"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Get ranges for consistent axes\n",
    "    all_x = pd.concat([m_to_Mpc(particle_df['x']) for particle_df in valid_particles])\n",
    "    all_y = pd.concat([m_to_Mpc(particle_df['y']) for particle_df in valid_particles])\n",
    "    all_z = pd.concat([m_to_Mpc(particle_df['z']) for particle_df in valid_particles])\n",
    "    \n",
    "    # Function to get color based on velocity\n",
    "    def get_color(velocity):\n",
    "        norm = plt.Normalize(-50, 50)\n",
    "        return plt.cm.coolwarm(norm(velocity))\n",
    "    \n",
    "    # Create each subplot\n",
    "    axes = []\n",
    "    for i, timestep in enumerate(timesteps):\n",
    "        ax = fig.add_subplot(1, len(timesteps), i+1, projection='3d')\n",
    "        axes.append(ax)\n",
    "        \n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        z_data = []\n",
    "        colors = []\n",
    "        \n",
    "        # Gather data for this timestep\n",
    "        for particle_df in valid_particles:\n",
    "            if timestep < len(particle_df):\n",
    "                x = m_to_Mpc(particle_df.iloc[timestep]['x'])\n",
    "                y = m_to_Mpc(particle_df.iloc[timestep]['y'])\n",
    "                z = m_to_Mpc(particle_df.iloc[timestep]['z'])\n",
    "                velocity = particle_df.iloc[timestep]['velocity']\n",
    "                x_data.append(x)\n",
    "                y_data.append(y)\n",
    "                z_data.append(z)\n",
    "                colors.append(get_color(velocity))\n",
    "        \n",
    "        # Plot data\n",
    "        ax.scatter(x_data, y_data, z_data, c=colors, s=10)\n",
    "        \n",
    "        # Set consistent axes limits\n",
    "        ax.set_xlim(all_x.min(), all_x.max())\n",
    "        ax.set_ylim(all_y.min(), all_y.max())\n",
    "        ax.set_zlim(all_z.min(), all_z.max())\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel('X Position [Mpc]', fontsize=14)\n",
    "        ax.set_ylabel('Y Position [Mpc]', fontsize=14)\n",
    "        ax.set_zlabel('Z Position [Mpc]', fontsize=14)\n",
    "        ax.set_title(f't = {timestep/10} Gyr', fontsize=14)\n",
    "        \n",
    "        # Set viewing angle for consistency\n",
    "        ax.view_init(elev=30, azim=45)\n",
    "    \n",
    "    # Add colorbar for velocity\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(-50, 50), cmap=plt.cm.coolwarm), \n",
    "                        cax=cbar_ax)\n",
    "    cbar.set_label('Velocity (km/s)')\n",
    "    \n",
    "    # Fix layout\n",
    "    plt.subplots_adjust(left=0.05, right=0.9, bottom=0.1, top=0.9, wspace=0.2)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('3d_position_snapshots.pdf', format='pdf')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "create_3d_position_snapshots(valid_particles, timesteps=[60, 95, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D animation of vr vs r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Assuming 'valid_particles' is a list of DataFrames with the required columns\n",
    "# Set plot limits based on the range of distances and velocities in the entire dataset\n",
    "all_distances = pd.concat([m_to_Mpc(particle_df['distances']) for particle_df in valid_particles])\n",
    "all_velocity = pd.concat([(particle_df['velocity'])/1000 for particle_df in valid_particles])\n",
    "ax.set_xlim(all_distances.min() - 1, all_distances.max() + 1)\n",
    "ax.set_ylim(all_velocity.min() - 1, all_velocity.max() + 1)\n",
    "\n",
    "# Initialize an empty scatter plot for updating later\n",
    "scatters = ax.scatter([], [], s=10)  # Adjust particle size with 's'\n",
    "\n",
    "# Animation function to update particle positions\n",
    "def animate(frame):\n",
    "    distances_data = []\n",
    "    velocity_data = []\n",
    "\n",
    "    # Update the data for each valid particle at the current frame (timestep)\n",
    "    for particle_df in valid_particles:\n",
    "        if frame < len(particle_df):  # Ensure frame is within particle data\n",
    "            distance = particle_df.iloc[frame]['distances']\n",
    "            velocity = particle_df.iloc[frame]['velocity']\n",
    "            distances_data.append(m_to_Mpc(distance))\n",
    "            velocity_data.append(velocity/1000)\n",
    "\n",
    "    # Plot updated positions of particles\n",
    "    scatters.set_offsets(np.c_[distances_data, velocity_data])\n",
    "\n",
    "    # Add labels, titles, or grid if needed (optional)\n",
    "    ax.set_xlabel('Distance [Mpc]', fontsize=14)\n",
    "    ax.set_ylabel('Velocity [km/s]', fontsize=14)\n",
    "    ax.set_title(f't = {frame/10} Gyr', fontsize=14)\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set up the animation\n",
    "anim = FuncAnimation(fig, animate, frames=138, interval=100)\n",
    "\n",
    "# Show the animation\n",
    "plt.show()\n",
    "#anim.save('vr_vs_r.mp4', writer='ffmpeg', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_velocity_distance_snapshots(valid_particles, timesteps=[40, 85, 130], figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Create side-by-side velocity vs distance visualizations\n",
    "    \"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(timesteps), figsize=figsize)\n",
    "    \n",
    "    # Get ranges for consistent axes\n",
    "    all_distances = pd.concat([m_to_Mpc(particle_df['distances']) for particle_df in valid_particles])\n",
    "    all_velocity = pd.concat([(particle_df['velocity'])/1000 for particle_df in valid_particles])\n",
    "    x_min, x_max = all_distances.min() - 0.2, all_distances.max() + 1\n",
    "    y_min, y_max = all_velocity.min() - 0.2, all_velocity.max() + 1\n",
    "    \n",
    "    # Create each subplot\n",
    "    for i, timestep in enumerate(timesteps):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        distances_data = []\n",
    "        velocity_data = []\n",
    "        \n",
    "        # Gather data for this timestep\n",
    "        for particle_df in valid_particles:\n",
    "            if timestep < len(particle_df):\n",
    "                distance = particle_df.iloc[timestep]['distances']\n",
    "                velocity = particle_df.iloc[timestep]['velocity']\n",
    "                distances_data.append(m_to_Mpc(distance))\n",
    "                velocity_data.append(velocity/1000)\n",
    "        \n",
    "        # Plot data\n",
    "        ax.scatter(distances_data, velocity_data, s=10)\n",
    "        \n",
    "        # Set consistent axes limits\n",
    "        #ax.set_xlim(x_min, x_max)\n",
    "        #ax.set_ylim(y_min, y_max) #this does not work very well...\n",
    "\n",
    "        ax.set_xlim(0.5, 2.75) # Hardcode for now...\n",
    "        ax.set_ylim(y_min, 200)\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel('Distance to the perturber[Mpc]')\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Radial velocity [km/s]')\n",
    "        ax.set_title(f't = {timestep/10} Gyr')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Fix layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('velocity_distance_snapshots.pdf', format='pdf')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "create_velocity_distance_snapshots(valid_particles, timesteps=[60, 95, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= Mpc_to_m(100) #100 mpc now\n",
    "# Create the new list of DataFrames with the modified x component\n",
    "valid_projections = []\n",
    "\n",
    "\n",
    "for particle_df in valid_particles:\n",
    "    # Make a copy of the original DataFrame to avoid modifying it directly\n",
    "    new_particle_df = particle_df.copy()\n",
    "    \n",
    "    # Add d to the x component\n",
    "    new_particle_df['r_primed'] = np.sqrt(new_particle_df['x']**2+ new_particle_df['y']**2 +new_particle_df['z']**2)\n",
    "    new_particle_df['z'] = new_particle_df['z'] + d\n",
    "    new_particle_df[\"distances\"] = np.sqrt(new_particle_df['x']**2+ new_particle_df['y']**2 +new_particle_df['z']**2)\n",
    "    #new_particle_df[\"theta\"] = np.arctan2(new_particle_df[\"y\"],(new_particle_df[\"x\"]))\n",
    "    #new_particle_df[\"phi\"] = np.arctan2(new_particle_df[\"z\"],(new_particle_df[\"x\"]))\n",
    "    #new_particle_df[\"v_los\"] = new_particle_df[\"velocity\"]*np.cos(new_particle_df[\"phi\"])*np.cos(new_particle_df[\"theta\"])\n",
    "    new_particle_df[\"v_los\"] = new_particle_df[\"vx\"]* new_particle_df[\"x\"]/new_particle_df[\"distances\"] + new_particle_df[\"vy\"]* new_particle_df[\"y\"]/new_particle_df[\"distances\"] +new_particle_df[\"vz\"]* new_particle_df[\"z\"]/new_particle_df[\"distances\"]\n",
    "    \n",
    "    # Append the modified DataFrame to the valid_projections list\n",
    "    valid_projections.append(new_particle_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d animation of x´, y´and z´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis for 3D plot\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Set plot limits based on the range of x, y, and z data across all particles\n",
    "all_x = pd.concat([particle_df['x'] for particle_df in valid_projections])\n",
    "all_y = pd.concat([particle_df['y'] for particle_df in valid_projections])\n",
    "all_z = pd.concat([particle_df['z'] for particle_df in valid_projections])\n",
    "ax.set_xlim(0, all_x.max()) #Force x axis to 0\n",
    "ax.set_ylim(all_y.min(), all_y.max())\n",
    "ax.set_zlim(all_z.min(), all_z.max())\n",
    "\n",
    "# Initialize an empty scatter plot for updating later\n",
    "scatters = ax.scatter([], [], [], s=10)  # Adjust particle size with 's'\n",
    "\n",
    "# Animation function to update particle positions in 3D\n",
    "def animate(frame):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    z_data = []\n",
    "    colors = []\n",
    "    # Update the data for each particle at the current frame (timestep)\n",
    "    for particle_df in valid_projections:\n",
    "        if frame < len(particle_df):  # Ensure frame is within particle data\n",
    "            x = particle_df.iloc[frame]['x']\n",
    "            y = particle_df.iloc[frame]['y']\n",
    "            z = particle_df.iloc[frame]['z']\n",
    "            velocity = particle_df.iloc[frame]['v_los']\n",
    "            x_data.append(x)\n",
    "            y_data.append(y)\n",
    "            z_data.append(z)\n",
    "            colors.append(get_color(velocity))\n",
    "\n",
    "    # Clear the previous scatter plot\n",
    "    ax.cla()\n",
    "\n",
    "    \n",
    "    # Reset the axis limits\n",
    "    ax.set_xlim(all_x.min(), all_x.max())\n",
    "    ax.set_ylim(all_y.min(), all_y.max())\n",
    "    ax.set_zlim(all_z.min(), all_z.max())\n",
    "\n",
    "    # Plot updated positions of particles in 3D, recreating the scatter plot\n",
    "    ax.scatter(x_data, y_data, z_data, c=colors, s=10)  # Use 'c' for color array\n",
    "    \n",
    "    # Set labels and title (optional)\n",
    "    ax.set_xlabel('X´ Position')\n",
    "    ax.set_ylabel('Y´ Position')\n",
    "    ax.set_zlabel('Z´ Position')\n",
    "    ax.set_title(f'Timestep {frame}')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set up the animation\n",
    "anim = FuncAnimation(fig, animate, frames=138, interval=10)\n",
    "\n",
    "# Show the animation\n",
    "plt.show()\n",
    "#anim.save('3D_animation_projection.gif', writer='imagemagick', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timestep_snapshots(valid_projections, timesteps=[40, 80, 120], figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Create side-by-side 3D visualizations with positions in Mpc\n",
    "    \"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Convert data to Mpc and get ranges for consistent axes\n",
    "    all_x_mpc = pd.concat([m_to_Mpc(particle_df['x']) for particle_df in valid_projections])\n",
    "    all_y_mpc = pd.concat([m_to_Mpc(particle_df['y']) for particle_df in valid_projections])\n",
    "    all_z_mpc = pd.concat([m_to_Mpc(particle_df['z']) for particle_df in valid_projections])\n",
    "    \n",
    "    # Function to get color based on velocity\n",
    "    def get_color(velocity):\n",
    "        norm = plt.Normalize(-50, 50)\n",
    "        return plt.cm.coolwarm(norm(velocity))\n",
    "    \n",
    "    # Create each subplot\n",
    "    axes = []\n",
    "    for i, timestep in enumerate(timesteps):\n",
    "        ax = fig.add_subplot(1, len(timesteps), i+1, projection='3d')\n",
    "        axes.append(ax)\n",
    "        \n",
    "        x_data_mpc = []\n",
    "        y_data_mpc = []\n",
    "        z_data_mpc = []\n",
    "        colors = []\n",
    "        \n",
    "        # Gather data for this timestep\n",
    "        for particle_df in valid_projections:\n",
    "            if timestep < len(particle_df):\n",
    "                x_mpc = m_to_Mpc(particle_df.iloc[timestep]['x'])\n",
    "                y_mpc = m_to_Mpc(particle_df.iloc[timestep]['y'])\n",
    "                z_mpc = m_to_Mpc(particle_df.iloc[timestep]['z'])\n",
    "                velocity = particle_df.iloc[timestep]['v_los']\n",
    "                x_data_mpc.append(x_mpc)\n",
    "                y_data_mpc.append(y_mpc)\n",
    "                z_data_mpc.append(z_mpc)\n",
    "                colors.append(get_color(velocity))\n",
    "        \n",
    "        # Plot data\n",
    "        scatter = ax.scatter(x_data_mpc, y_data_mpc, z_data_mpc, c=colors, s=15)\n",
    "        \n",
    "        # Set consistent axes limits\n",
    "        ax.set_xlim(all_x_mpc.min(), all_x_mpc.max())\n",
    "        ax.set_ylim(all_y_mpc.min(), all_y_mpc.max())\n",
    "        ax.set_zlim(all_z_mpc.min(), all_z_mpc.max())\n",
    "        \n",
    "        # Labels and title - convert timestep to time in Gyr\n",
    "        time_in_gyr = timestep / 10  # Assuming 1 timestep = 0.1 Gyr\n",
    "        ax.set_xlabel('X´ Position [Mpc]')\n",
    "        ax.set_ylabel('Y´ Position [Mpc]')\n",
    "        ax.set_zlabel('Z´ Position [Mpc]')\n",
    "        ax.set_title(f't = {time_in_gyr} Gyr')\n",
    "        \n",
    "        # Set viewing angle for consistency\n",
    "        ax.view_init(elev=30, azim=45)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Add colorbar for velocity\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(-50, 50), cmap=plt.cm.coolwarm), \n",
    "                        cax=cbar_ax)\n",
    "    cbar.set_label('Velocity (km/s)')\n",
    "    \n",
    "    # Fix layout issue by using subplots_adjust instead of tight_layout\n",
    "    plt.subplots_adjust(left=0.05, right=0.9, bottom=0.1, top=0.9, wspace=0.2)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('timestep_snapshots_mpc.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('timestep_snapshots_mpc.pdf', format='pdf')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "create_timestep_snapshots(valid_projections, timesteps=[60, 95, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D animation of line of sight velocity vs distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and axis for the plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Assumingn valid_projections' is a list of DataFrames with the required columns\n",
    "# Set plot limits based on the range of distances and velocities in the entire dataset\n",
    "all_distances = pd.concat([m_to_Mpc(particle_df['distances']) for particle_df in valid_projections])\n",
    "all_velocity = pd.concat([particle_df['v_los']/1000 for particle_df in valid_projections])\n",
    "ax.set_xlim(all_distances.min() - 1, all_distances.max() + 1)\n",
    "ax.set_ylim(all_velocity.min() - 1, all_velocity.max() + 1)\n",
    "\n",
    "# Initialize an empty scatter plot for updating later\n",
    "scatters = ax.scatter([], [], s=10, alpha=0.5)  # Adjust particle size with 's\n",
    "\n",
    "# Animation function to update particle positions\n",
    "def animate(frame):\n",
    "    #ax.clear()  # Clear the axis for each frame\n",
    "    distances_data = []\n",
    "    velocity_data = []\n",
    "\n",
    "    # Update the data for each valid particle at the current frame (timestep)\n",
    "    for particle_df in valid_projections:\n",
    "        if frame < len(particle_df):  # Ensure frame is within particle data\n",
    "            distance = particle_df.iloc[frame]['distances']\n",
    "            velocity = particle_df.iloc[frame]['v_los']\n",
    "            distances_data.append(m_to_Mpc(distance))\n",
    "            velocity_data.append(velocity/1000)\n",
    "\n",
    "    # Plot updated positions of particles\n",
    "    scatters.set_offsets(np.c_[distances_data, velocity_data])\n",
    "    \n",
    "\n",
    "    # Add labels, titles, or grid if needed (optional)\n",
    "    ax.set_xlabel('Distance to the Observer [Mpc]')\n",
    "    ax.set_ylabel('Line of Sight Velocity [km/s]')\n",
    "    ax.set_title(f't = {frame/10} Gyr')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set up the animation\n",
    "anim = FuncAnimation(fig, animate, frames=138, interval=100)\n",
    "\n",
    "# Show the animation\n",
    "#plt.show()\n",
    "anim.save('v_los.mp4', writer='ffmpeg', fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hubble_snapshots(valid_projections, timesteps=[40, 80, 120], figsize=(15, 5)):\n",
    "    \"\"\"\n",
    "    Create side-by-side Hubble diagram visualizations\n",
    "    \"\"\"\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(timesteps), figsize=figsize)\n",
    "    \n",
    "    # Get ranges for consistent axes\n",
    "    all_distances = pd.concat([m_to_Mpc(particle_df['distances']) for particle_df in valid_projections])\n",
    "    all_velocity = pd.concat([particle_df['v_los']/1000 for particle_df in valid_projections])\n",
    "    x_min, x_max = all_distances.min() - 1, all_distances.max() + 1\n",
    "    y_min, y_max = all_velocity.min() - 1, all_velocity.max() + 1\n",
    "    \n",
    "    # Create each subplot\n",
    "    for i, timestep in enumerate(timesteps):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        distances_data = []\n",
    "        velocity_data = []\n",
    "        \n",
    "        # Gather data for this timestep\n",
    "        for particle_df in valid_projections:\n",
    "            if timestep < len(particle_df):\n",
    "                distance = particle_df.iloc[timestep]['distances']\n",
    "                velocity = particle_df.iloc[timestep]['v_los']\n",
    "                distances_data.append(m_to_Mpc(distance))\n",
    "                velocity_data.append(velocity/1000)\n",
    "        \n",
    "        # Plot data\n",
    "        ax.scatter(distances_data, velocity_data, s=10, alpha=0.5)\n",
    "        \n",
    "        # Set consistent axes limits\n",
    "        ax.set_xlim(97, 103)\n",
    "        ax.set_ylim(-200, 200)\n",
    "        \n",
    "        # Labels and title\n",
    "        ax.set_xlabel('Distance to Observer [Mpc]')\n",
    "        ax.set_ylabel('Line of Sight Velocity [km/s]') if i == 0 else None\n",
    "        ax.set_title(f't = {timestep/10} Gyr')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Fix layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('hubble_snapshots.pdf', format='pdf')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "create_hubble_snapshots(valid_projections, timesteps=[60, 95, 130])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "df_final_snapshot = pd.DataFrame()\n",
    "for i in tqdm(range(len(valid_projections))):\n",
    "    df_new = valid_projections[i].iloc[-1].to_frame().T  # Convert Series to DataFrame and transpose to make it a row\n",
    "    df_final_snapshot = pd.concat([df_final_snapshot, df_new], ignore_index=True)\n",
    "#df_final_snapshot.drop('Unnamed: 0', axis=1, inplace=True) \n",
    "df_final_snapshot['theta'] = np.arccos(df_final_snapshot['z']/df_final_snapshot['distances'])\n",
    "#df_final_snapshot.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert x,y,z,distances to Mpc\n",
    "df_final_snapshot['x'] = m_to_Mpc(df_final_snapshot['x'])\n",
    "df_final_snapshot['y'] = m_to_Mpc(df_final_snapshot['y'])\n",
    "df_final_snapshot['z'] = m_to_Mpc(df_final_snapshot['z'])\n",
    "df_final_snapshot['distances'] = m_to_Mpc(df_final_snapshot['distances'])\n",
    "df_final_snapshot['r_primed'] = m_to_Mpc(df_final_snapshot['r_primed'])\n",
    "#convert vx,vy,vz, velocity, v_los to km/s\n",
    "df_final_snapshot['vx'] = df_final_snapshot['vx']/1000\n",
    "df_final_snapshot['vy'] = df_final_snapshot['vy']/1000\n",
    "df_final_snapshot['vz'] = df_final_snapshot['vz']/1000\n",
    "df_final_snapshot['velocity'] = df_final_snapshot['velocity']/1000\n",
    "df_final_snapshot['v_los'] = df_final_snapshot['v_los']/1000\n",
    "df_final_snapshot.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_snapshot.to_csv('final_snapshot_units.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_snapshot = pd.read_csv(\"final_snapshot_units.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_data(df_final_snapshot):\n",
    "    mock_data_df = pd.DataFrame()\n",
    "    mock_data_df[\"s\"] = df_final_snapshot[\"distances\"]\n",
    "    mock_data_df[\"v_los\"] = df_final_snapshot[\"v_los\"]\n",
    "    mock_data_df[\"theta\"] = np.arccos(df_final_snapshot['z']/df_final_snapshot['distances'])\n",
    "    columns_to_randomize = ['s', \"v_los\"]  \n",
    "    # 100kpc in D and 20 km/s in v_los\n",
    "    mock_data_df[\"s_error\"] = 0.1\n",
    "    mock_data_df[\"v_los_error\"] = np.sqrt(20**2 + 50**2)\n",
    "\n",
    "    # randomize the data with the errors\n",
    "    for column in columns_to_randomize:\n",
    "        mean_values = mock_data_df[column]\n",
    "        std_dev = mock_data_df[column + '_error']\n",
    "        mock_data_df[column] = np.random.normal(loc=mean_values, scale=std_dev, size=len(mock_data_df))\n",
    "    mock_data_df[\"v_los_error\"] = 20\n",
    "    return mock_data_df\n",
    "\n",
    "#mock_data_df = randomize_data(df_final_snapshot)\n",
    "mock_data_df = randomize_data(df_final_snapshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_data_df.to_csv('mock_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distances vs v_los for the mock data and the final snapshot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(mock_data_df[\"s\"],mock_data_df[\"v_los\"],\".\",label = \"Mock Data\")\n",
    "plt.plot((df_final_snapshot[\"distances\"]),df_final_snapshot[\"v_los\"],\".\", label = \"Simulation Data\")\n",
    "plt.xlabel(\"Distance from the Observer [Mpc]\")\n",
    "plt.ylabel(\"Line of Sight Velocity [km/s]\")\n",
    "plt.legend()\n",
    "#plt.title(\"Mock Data vs True Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"Mock_vs_True.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow ties comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_model(s_primed,theta,t,D,mp,Omega_lambda):\n",
    "    t = Gyr_to_s(t) # s\n",
    "    D = Mpc_to_m(D) # m\n",
    "    s_primed = Mpc_to_m(s_primed) # m\n",
    "    r_primed = np.sqrt(D**2 + s_primed**2 - 2*D*s_primed*np.cos(theta)) # m \n",
    "    term_1 = (r_primed/t)\n",
    "    term_2 = np.sqrt((const.G.value*mp)/r_primed)\n",
    "    argument = s_primed * np.sin(theta) / r_primed\n",
    "    argument = np.clip(argument, -1.0, 1.0) #DISCUSS THIS\n",
    "    theta_primed = np.arcsin(argument)\n",
    "    #theta_primed = np.arcsin(s_primed * np.sin(theta) / r_primed)\n",
    "    expanssion_term = 0.31*Omega_lambda*r_primed/t\n",
    "    mask = s_primed*np.cos(theta) < D\n",
    "    theta_primed[mask] = np.pi - theta_primed[mask]\n",
    "    term_3 = np.cos(theta_primed-theta)\n",
    "    return (1.2*term_1 - 1.1*term_2+expanssion_term)*term_3 / 1000 #km/s\n",
    "\n",
    "# Lyndell-Bell\n",
    "def Lyndell_calc(r_primed,t,mp,Omega_lambda):\n",
    "    r_primed = Mpc_to_m(r_primed)\n",
    "    t = Gyr_to_s(t)\n",
    "    term_1 = (r_primed/t)\n",
    "    term_2 = np.sqrt((const.G.value*mp)/r_primed)\n",
    "    expanssion_term = 0.31*Omega_lambda*r_primed/t\n",
    "    #print(theta_primed,theta,term_1,term_2,term_3)\n",
    "    return (1.2*term_1 - 1.1*term_2 + expanssion_term) / 1000\n",
    "def Lyndell_Bell_no_expanssion(r_primed,t,mp):\n",
    "    r_primed = Mpc_to_m(r_primed)\n",
    "    t = Gyr_to_s(t)\n",
    "    term_1 = (r_primed/t)\n",
    "    term_2 = np.sqrt((const.G.value*mp)/r_primed)\n",
    "    #print(theta_primed,theta,term_1,term_2,term_3)\n",
    "    return (1.2*term_1 - 1.1*term_2) / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_Lyndell(r_primed,v,t,mp,Omega_lambda):\n",
    "    plt.plot(r_primed,v,\".\",label= \"Simulation\")\n",
    "    plt.plot(r_primed,Lyndell_calc(r_primed,t,mp,Omega_lambda),\".\",label= \"Lynden-Bell\")\n",
    "    plt.xlabel('Distance [Mpc]')\n",
    "    plt.ylabel('Velocity [km/s]')\n",
    "    #plt.title('Modified Hubble Flow', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "compare_Lyndell(df_final_snapshot[\"r_primed\"],df_final_snapshot[\"velocity\"],13.7,M,0.69)\n",
    "plt.savefig(\"Lynden_Bell.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def compare_bow_ties(d,v_los,theta,t,D,M,Omega_lambda):\n",
    "    plt.plot(d, v_model(d,theta,t,100,M,Omega_lambda), '.', label='Analytical expression')\n",
    "    plt.plot(d, v_los, '.', label='Simulation')\n",
    "    #plt.plot(df_final_snapshot['distances_Mpc'], df_final_snapshot['v_los_analytical_wrong'], '.', label='MCMC parameters')\n",
    "    plt.xlabel('Distance [Mpc]')\n",
    "    plt.ylabel('Line of Sight Velocity [km/s]')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.title('Distances vs line of sight velocities')\n",
    "    plt.show()\n",
    "#plt.savefig('bow_ties_simulation_presentation.png')\n",
    "compare_bow_ties(df_final_snapshot['distances'],df_final_snapshot['v_los'],\n",
    "df_final_snapshot['theta'],13.7,100,M,0.69)\n",
    "plt.savefig(\"bow_tie.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of distances in Mpc\n",
    "r_primed_range = np.linspace(0.01, 4, 1000)\n",
    "\n",
    "# Define different times in Gyr\n",
    "times = [2, 6, 13, 50]\n",
    "\n",
    "# Plot the analytical expression for different times\n",
    "plt.figure(figsize=(10, 6))\n",
    "for t in times:\n",
    "    v_analytical = Lyndell_calc(r_primed_range, t, M,0.69)\n",
    "    v_no_expansion = Lyndell_Bell_no_expanssion(r_primed_range, t, M)\n",
    "    plt.plot(r_primed_range, v_analytical, label=f't = {t} Gyr')\n",
    "    plt.plot(r_primed_range, v_no_expansion, linestyle=':', color=plt.gca().lines[-1].get_color(), alpha=0.5)\n",
    "\n",
    "# Add a dotted line around y = 0\n",
    "plt.axhline(y=0, color='black', linestyle='--', label='turnaround radius')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Distance [Mpc]')\n",
    "plt.ylabel('Velocity [km/s]')\n",
    "#plt.title('Velocity vs Distance for Different Times', fontsize=16)\n",
    "plt.legend()\n",
    "plt.ylim(-200, 400)\n",
    "#plt.title('Velocity vs Distance for Different Times')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"Lyndell_Bell_varying_time.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of distances in Mpc\n",
    "r_primed_range = np.linspace(0.01, 4, 1000)\n",
    "\n",
    "# Define different masses in solar masses\n",
    "masses = [0, 1e12, 5e12, 1e13, 5e13]\n",
    "\n",
    "# Plot the analytical expression for different masses\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, mass in enumerate(masses):\n",
    "    mass_in_kg = mass * const.M_sun.value\n",
    "    v_analytical = Lyndell_calc(r_primed_range, 13.7, mass_in_kg, 0.69)\n",
    "    \n",
    "    # Special formatting for M=0 case\n",
    "    if mass == 0:\n",
    "        plt.plot(r_primed_range, v_analytical, color='black', linestyle='dashdot', alpha=0.5, \n",
    "                 label='Unperturbed Hubble flow')\n",
    "    else:\n",
    "        plt.plot(r_primed_range, v_analytical, label=f'M = {mass:.1e} $M_{{\\odot}}$')\n",
    "\n",
    "# Add a dotted line around y = 0\n",
    "plt.axhline(y=0, color='black', linestyle='--', label='turnaround radius')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Distance [Mpc]')\n",
    "plt.ylabel('Velocity [km/s]')\n",
    "plt.legend()\n",
    "plt.ylim(-700, 400)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"Lyndell_Bell_masses.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Function - MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# OLD STUFF CHECK UNITS \n",
    "def log_prior(phi):\n",
    "    t, mp, Omega_lambda,hyper = phi\n",
    "    if 5 < t < 20 and 10 < mp < 16 and 0.5 < Omega_lambda < 1 and hyper > 0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "    \n",
    "def log_likelihood(phi, s, s_error, theta, v_los, v_los_error,D):\n",
    "    t = Gyr_to_s(phi[0])\n",
    "    mp = (10**phi[1])*const.M_sun.value\n",
    "    Omega_lambda = phi[2]\n",
    "    hyper = phi[3] * 1000 #rescale hyperparameter\n",
    "    \n",
    "    num_samples = 1000 # test different values\n",
    "    total_log_likelihood = 0.0\n",
    "    #probs_list = []\n",
    "    #v_list = []\n",
    "    for i in range(len(s)):\n",
    "        s_primed_samples = np.random.normal(s[i], s_error[i], num_samples)\n",
    "        v_los_samples = v_model(s_primed_samples, theta[i], t,D, mp, Omega_lambda) #Check the value of D\n",
    "        # Calculate the probability of obtaining v_los_samples from a normal distribution with mean v_los[i] and standard deviation v_los_error[i]\n",
    "        log_probs = norm.logpdf(\n",
    "            v_los_samples, loc=v_los[i], scale=math.sqrt(v_los_error[i]**2+hyper**2)\n",
    "        )\n",
    "        #print(log_probs)\n",
    "        #probs_list.append(log_probs)\n",
    "        #v_list.append(v_los_samples)\n",
    "\n",
    "        log_likelihood_i = logsumexp(log_probs) - np.log(num_samples)\n",
    "        #print(log_likelihood_i)\n",
    "        total_log_likelihood += log_likelihood_i\n",
    "    return total_log_likelihood\n",
    "\n",
    "\n",
    "def log_prob(phi, s, s_error,theta, v_los, v_los_error,D):\n",
    "    # Compute the log-prior\n",
    "    log_prior_value = log_prior(phi)\n",
    "    if not np.isfinite(log_prior_value):\n",
    "        #print(f\"log_prior returned {log_prior_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    # Compute the log-likelihood\n",
    "    log_likelihood_value = log_likelihood(phi, s, s_error, theta, v_los, v_los_error,D)\n",
    "    if not np.isfinite(log_likelihood_value):\n",
    "        #print(f\"log_likelihood returned {log_likelihood_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    return log_prior_value + log_likelihood_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# FIXX! add D to the model, why did I use 5Mpc everywhere???\n",
    "def v_model(s_primed, theta, t, mp, Omega_lambda):\n",
    "    D = Mpc_to_m(5)  # Convert 5 Mpc to meters\n",
    "    # Ensure s_primed and theta are arrays\n",
    "    s_primed = np.asarray(s_primed)\n",
    "    theta = np.asarray(theta)\n",
    "\n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-10\n",
    "    s_primed = np.where(s_primed == 0, epsilon, s_primed)\n",
    "    r_primed_squared = D**2 + s_primed**2 - 2 * D * s_primed * np.cos(theta)\n",
    "    r_primed_squared = np.where(r_primed_squared < 0, epsilon, r_primed_squared)\n",
    "    r_primed = np.sqrt(r_primed_squared)\n",
    "\n",
    "    term_1 = r_primed / t\n",
    "    term_2 = np.sqrt((const.G.value * mp) / r_primed)\n",
    "\n",
    "    argument = s_primed * np.sin(theta) / r_primed\n",
    "    argument = np.clip(argument, -1.0, 1.0)  # Ensure valid domain for arcsin\n",
    "    theta_primed = np.arcsin(argument)\n",
    "    expanssion_term = 0.31 * Omega_lambda * r_primed / s_primed\n",
    "\n",
    "    mask = m_to_Mpc(s_primed * np.cos(theta)) < 5\n",
    "    theta_primed = np.where(mask, np.pi - theta_primed, theta_primed)\n",
    "\n",
    "    term_3 = np.cos(theta_primed - theta)\n",
    "    result = (1.2 * term_1 - 1.1 * term_2 + expanssion_term) * term_3\n",
    "\n",
    "    # Handle invalid values in result\n",
    "    result = np.where(np.isnan(result) | np.isinf(result), 0.0, result)\n",
    "\n",
    "    # If inputs were scalar, return scalar\n",
    "    if np.isscalar(s_primed) and result.size == 1:\n",
    "        return result.item()\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "def log_prior(phi):\n",
    "    t, mp_log10, Omega_lambda, hyper = phi\n",
    "    if 5 < t < 20 and 10 < mp_log10 < 14 and 0.5 < Omega_lambda < 1 and hyper > 0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def deterministic_log_likelihood(phi, s, s_error, theta, v_los, v_los_error):\n",
    "    t = Gyr_to_s(phi[0])\n",
    "    mp = (10 ** phi[1]) * const.M_sun.value\n",
    "    Omega_lambda = phi[2]\n",
    "    hyper = phi[3]\n",
    "\n",
    "    total_log_likelihood = 0.0\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        # Define the integrand function for each data point\n",
    "        def integrand(s_primed):\n",
    "            # Add a small epsilon to s_primed to avoid division by zero\n",
    "            s_primed = np.where(s_primed == 0, 1e-10, s_primed)\n",
    "\n",
    "            # PDF of s_primed given observed s_i\n",
    "            p_s = norm.pdf(s_primed, loc=s[i], scale=s_error[i])\n",
    "\n",
    "            # Compute the model velocity for given s_primed\n",
    "            v_model_i = v_model(s_primed, theta[i], t, mp, Omega_lambda)\n",
    "\n",
    "            # Total observational error\n",
    "            total_error = math.sqrt(v_los_error[i] ** 2 + hyper ** 2)\n",
    "\n",
    "            # PDF of v_los_i given model velocity\n",
    "            p_v = norm.pdf(v_los[i], loc=v_model_i, scale=total_error)\n",
    "\n",
    "            # Return the product of the PDFs\n",
    "            integrand_value = p_s * p_v\n",
    "            return integrand_value\n",
    "\n",
    "        # Determine integration limits based on model and hyper\n",
    "        s_mean = s[i]\n",
    "        s_std = s_error[i]\n",
    "        epsilon = 1e-10  # Small positive number to avoid zero\n",
    "        lower_limit = max(s_mean - 50 * s_std, epsilon)\n",
    "        upper_limit = s_mean + 50 * s_std  # Increased from 5 to 50\n",
    "\n",
    "        # For debugging at index 0\n",
    "        if i == 0:\n",
    "            s_primed_test = np.linspace(lower_limit, upper_limit, 1000)\n",
    "            integrand_test = integrand(s_primed_test)\n",
    "\n",
    "            # Check where the integrand is non-zero\n",
    "            non_zero_indices = integrand_test > 1e-300  # Use a small threshold\n",
    "            print(f\"Index 0: Number of non-zero integrand values: {np.sum(non_zero_indices)}\")\n",
    "\n",
    "            if np.sum(non_zero_indices) == 0:\n",
    "                print(\"Integrand is zero over the entire range at index 0.\")\n",
    "                return -np.inf\n",
    "\n",
    "        # Perform the numerical integration\n",
    "        try:\n",
    "            with np.errstate(divide='ignore', invalid='ignore')\n",
    "                integral_value, integral_error = quad(\n",
    "                    integrand, lower_limit, upper_limit, limit=500, epsabs=1e-8, epsrel=1e-8\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Integration error at index {i}: {e}\")\n",
    "            return -np.inf\n",
    "\n",
    "        # Check if the integral_value is positive\n",
    "        if integral_value <= 0 or not np.isfinite(integral_value):\n",
    "            print(f\"Integral value non-positive at index {i}: {integral_value}\")\n",
    "            return -np.inf\n",
    "\n",
    "        # Add the log of the integral to the total log likelihood\n",
    "        total_log_likelihood += np.log(integral_value)\n",
    "\n",
    "    return total_log_likelihood\n",
    "\n",
    "def monte_carlo_log_likelihood(phi, s, s_error, theta, v_los, v_los_error, num_samples):\n",
    "    t = Gyr_to_s(phi[0])\n",
    "    mp = (10 ** phi[1]) * const.M_sun.value\n",
    "    Omega_lambda = phi[2]\n",
    "    hyper = phi[3]\n",
    "\n",
    "    total_log_likelihood = 0.0\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        # Generate samples from normal distribution\n",
    "        s_primed_samples = np.random.normal(s[i], s_error[i], num_samples)\n",
    "        s_primed_samples = np.where(s_primed_samples <= 0, 1e-10, s_primed_samples)  # Avoid zero or negative s_primed\n",
    "\n",
    "        v_los_samples = v_model(s_primed_samples, theta[i], t, mp, Omega_lambda)\n",
    "\n",
    "        # Total observational error\n",
    "        total_error = math.sqrt(v_los_error[i] ** 2 + hyper ** 2)\n",
    "\n",
    "        log_probs = norm.logpdf(\n",
    "            v_los[i], loc=v_los_samples, scale=total_error\n",
    "        )\n",
    "\n",
    "        # Use logsumexp for numerical stability\n",
    "        max_log_prob = np.max(log_probs)\n",
    "        log_likelihood_i = max_log_prob + np.log(np.sum(np.exp(log_probs - max_log_prob))) - np.log(num_samples)\n",
    "\n",
    "        total_log_likelihood += log_likelihood_i\n",
    "\n",
    "    return total_log_likelihood\n",
    "\n",
    "\n",
    "phi = [13.7, np.log10(M / const.M_sun.value), 0.7, 5e4\n",
    "]  # t, log10(mp/M_sun), Omega_lambda, hyper\n",
    "s = mock_data_df[\"s\"].values\n",
    "s_error = mock_data_df[\"s_error\"].values\n",
    "v_los = mock_data_df[\"v_los\"].values\n",
    "v_los_error = mock_data_df[\"v_los_error\"].values\n",
    "theta = mock_data_df[\"theta\"].values\n",
    "\n",
    "\n",
    "\n",
    "# Compute deterministic log likelihood\n",
    "det_log_likelihood = deterministic_log_likelihood(\n",
    "    phi, s, s_error, theta, v_los, v_los_error\n",
    ")\n",
    "\n",
    "# Check if the deterministic log likelihood is finite\n",
    "if np.isfinite(det_log_likelihood):\n",
    "    # Monte Carlo sample sizes to test\n",
    "    sample_sizes = [100, 500, 1000, 5000, 10000]\n",
    "\n",
    "    for num_samples in sample_sizes:\n",
    "        mc_log_likelihood = monte_carlo_log_likelihood(\n",
    "            phi, s, s_error, theta, v_los, v_los_error, num_samples\n",
    "        )\n",
    "        difference = abs(det_log_likelihood - mc_log_likelihood)\n",
    "        print(\n",
    "            f\"Samples: {num_samples}, Monte Carlo Log Likelihood: {mc_log_likelihood:.6f}, \"\n",
    "            f\"Deterministic Log Likelihood: {det_log_likelihood:.6f}, \"\n",
    "            f\"Difference: {difference:.6f}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Deterministic log likelihood is -inf; cannot compare to Monte Carlo integration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "samples = [100, 500, 1000, 5000, 10000]\n",
    "differences = [1.385126, 0.170479, 0.115900, 0.023406, 0.019328]\n",
    "deterministic_log_likelihood = -12415.741784\n",
    "\n",
    "# Compute percentage differences\n",
    "percentage_differences = [(diff / abs(deterministic_log_likelihood)) * 100 for diff in differences]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(samples, percentage_differences, marker='o', linestyle='-', color='b')\n",
    "\n",
    "# Formatting\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Number of Samples\")\n",
    "plt.ylabel(\"Percentage Difference (%)\")\n",
    "#plt.title(\"Percentage Difference of Monte Carlo Log Likelihood to Deterministic\")\n",
    "plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('percentage_difference_monte_carlo.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner  \n",
    "import math\n",
    "\n",
    "s = mock_data_df[\"s\"]          # Array of observed `s` values\n",
    "s_error = mock_data_df[\"s_error\"]       # Array of errors associated with `s`\n",
    "v_los = mock_data_df[\"v_los\"]       # Array of observed line-of-sight velocities\n",
    "v_los_error = mock_data_df[\"v_los_error\"] \n",
    "theta = mock_data_df[\"theta\"]    # Array of errors associated with `v_los`\n",
    "\n",
    "# Initial guesses for log parameters (log10 values of t and mp, and Omega_lambda)\n",
    "initial_phi = [(13.7), np.log10(M/const.M_sun.value), 0.7,50]  \n",
    "# MCMC parameters\n",
    "ndim = 4             # Number of parameters in `phi`\n",
    "nwalkers = 16         # Reduced number of walkers\n",
    "nsteps = 3500 # 50 x autocorrelation time\n",
    "nburn = 300\n",
    "def run_mcmc(s,s_error,v_los,v_los_error,theta,D,initial_phi,ndim,nwalkers,nsteps,nburn):\n",
    "\n",
    "    # Initial positions of walkers around the initial guess\n",
    "    p0 = initial_phi + 1e-2 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "    # Initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob, args=(s, s_error,theta, v_los, v_los_error,D))\n",
    "\n",
    "    # Run burn-in phase to allow walkers to explore the parameter space\n",
    "    print(\"Running burn-in...\")\n",
    "    state = sampler.run_mcmc(p0, nburn, progress=True)\n",
    "    sampler.reset()\n",
    "\n",
    "    # Run the main MCMC sampling\n",
    "    print(\"Running MCMC sampling...\")\n",
    "    sampler.run_mcmc(state, nsteps, progress=True)\n",
    "\n",
    "    # Flatten the chain to get a 2D array of samples\n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    return sampler, samples\n",
    "D = Mpc_to_m(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler_TEST, sampler\n",
    "sampler_TEST4 , samples_TEST4 = run_mcmc(s, s_error, v_los, v_los_error, theta, D, initial_phi, ndim, nwalkers, nsteps, nburn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Trace Plots\n",
    "def plot_corner(sampler,fname,M_True):\n",
    "    plt.rcParams.update({'font.size': 14})  # Increase font size globally\n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    fig = corner.corner(samples, labels=[\"t [Gyr]\", \"log(mp)\", \"Omega_lambda\",\"hyperparameter [km/s]\"],\n",
    "                        truths=[13.7, np.log10(M_True/const.M_sun.value), 0.7,50])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "def plot_traces(sampler):\n",
    "    samples = sampler.get_chain(flat=False)\n",
    "    # Retrieve the number of walkers and dimensions from the samples array\n",
    "    nsteps, nwalkers, ndim = samples.shape\n",
    "\n",
    "    # Define labels for your parameters (adjust as needed)\n",
    "    parameter_labels = [\"t\", \"log(mp)\", \"Omega_lambda\",\"hyper\"]\n",
    "\n",
    "    # Create a figure for each parameter\n",
    "    for i in range(ndim):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for j in range(nwalkers):\n",
    "            plt.plot(samples[:, j, i], alpha=0.5)\n",
    "        plt.title(f\"Trace Plot for Parameter '{parameter_labels[i]}'\")\n",
    "        plt.xlabel(\"Step Number\")\n",
    "        plt.ylabel(f\"Value of '{parameter_labels[i]}'\")\n",
    "        plt.grid(True)\n",
    "        #plt.savefig(f\"new_trace_plot_{parameter_labels[i]}.png\")\n",
    "        plt.show()\n",
    "# Acceptance rates\n",
    "def acceptance_fraction_calc(sampler):\n",
    "    print(\"Mean acceptance fraction:\", np.mean(sampler.acceptance_fraction))\n",
    "    try:\n",
    "        tau = sampler.get_autocorr_time()\n",
    "        print(\"Autocorrelation time:\", tau)\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute autocorrelation time:\", e)\n",
    "def plot_log_probabilities(sampler, nwalkers):\n",
    "    log_prob_values0 = sampler.get_log_prob()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(nwalkers):\n",
    "        plt.plot(log_prob_values0[:, i], alpha=0.5)\n",
    "    plt.xlabel(\"Step Number\")\n",
    "    plt.ylabel(\"Log Probability\")\n",
    "    plt.title(\"Log Probability as a Function of Step Number\")\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(\"log_probabilities_Test2.png\")\n",
    "    plt.show()\n",
    "\n",
    "def run_diagnostic_plots(sampler,nwalkers,fname,M_True):\n",
    "    plot_corner(sampler,fname,M_True)\n",
    "    plot_traces(sampler)\n",
    "    acceptance_fraction_calc(sampler)\n",
    "    plot_log_probabilities(sampler,nwalkers)\n",
    "#run_diagnostic_plots(sampler_TEST4,nwalkers,\"corner_plot_TEST4.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing end values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_prob_values = sampler.get_log_prob()\n",
    "samples = sampler.get_chain(flat=True)  # Flattened samples (if necessary)\n",
    "\n",
    "# Find the index of the maximum log-probability\n",
    "flat_index = np.argmax(log_prob_values)  # Index in flattened array\n",
    "\n",
    "# Extract best parameters\n",
    "best_params = samples[flat_index]\n",
    "\n",
    "print(\"Best-fitting parameters:\", best_params)\n",
    "print(\"Best log-probability:\", log_prob_values.flatten()[flat_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_los_mcmc = v_model(mock_data_df[\"s\"],mock_data_df[\"theta\"],Gyr_to_s(best_params[0]), Mpc_to_m(100),(10**best_params[1])*const.M_sun, best_params[2])\n",
    "plt.plot(m_to_Mpc(mock_data_df[\"s\"]),v_los_mcmc,\".\",label = \"MCMC params\")\n",
    "#plt.plot(m_to_Mpc(mock_data_df[\"s\"]),mock_data_df[\"v_los\"],\".\",label = \"Mock data\")\n",
    "plt.plot(m_to_Mpc(df_final_snapshot[\"distances\"]),df_final_snapshot[\"v_los\"],\".\",label = \"Simulation data\")\n",
    "plt.xlabel(\"Distance from Earth [Mpc]\")\n",
    "plt.ylabel(\"Line of Sight Velocity [m/s]\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"v_los_mcmc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uchuu Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "fname = \"uchuu_mock_data_Pedro.hdf5\"\n",
    "M_virs = {\n",
    "    0: 2.136108650723353906e+13,\n",
    "    1: 1.808385001476232812e+13,\n",
    "    2: 1.335104812518452930e+13,\n",
    "    3: 1.062001771479185156e+13,\n",
    "    4: 1.027310304103926758e+13,\n",
    "    5: 9.827280779450841797e+12,\n",
    "    6: 9.594036020076763672e+12,\n",
    "    7: 7.214348981399468750e+12\n",
    "}\n",
    "with h5py.File(fname, 'r') as hdf:\n",
    "    \n",
    "    def print_structure(name, obj):\n",
    "        \"\"\"Callback to print the structure of the HDF5 file.\"\"\"\n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"Group: {name}\")\n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            print(f\"Dataset: {name} | Shape: {obj.shape} | Dtype: {obj.dtype}\")\n",
    "    \n",
    "    hdf.visititems(print_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5py_file(i):\n",
    "    with h5py.File(fname, 'r') as hdf:\n",
    "        s_xs = hdf[f'system_{i}']['particle_data']['x'][:]\n",
    "        s_ys = hdf[f'system_{i}']['particle_data']['y'][:]\n",
    "        s_zs = hdf[f'system_{i}']['particle_data']['z'][:]\n",
    "        s_vxs = hdf[f'system_{i}']['particle_data']['vx'][:]\n",
    "        s_vys = hdf[f'system_{i}']['particle_data']['vy'][:]\n",
    "        s_vzs = hdf[f'system_{i}']['particle_data']['vz'][:]\n",
    "        s_s = hdf[f'system_{i}']['particle_data']['d'][:]\n",
    "        s_s_true = hdf[f'system_{i}']['particle_data']['d_true'][:]\n",
    "        s_s_error = hdf[f'system_{i}']['particle_data']['d_error'][:]\n",
    "        s_v_los_true = hdf[f'system_{i}']['particle_data']['v_true'][:]\n",
    "        s_v_los = hdf[f'system_{i}']['particle_data']['v'][:]\n",
    "        s_v_los_error = hdf[f'system_{i}']['particle_data']['v_error'][()]\n",
    "        s_theta = hdf[f'system_{i}']['particle_data']['theta'][:]\n",
    "        D = hdf[f'system_{i}']['perturber_data']['d'][()]\n",
    "        v_pert = hdf[f'system_{i}']['perturber_data']['v'][()]\n",
    "        theta_pert = hdf[f'system_{i}']['perturber_data']['theta'][()]\n",
    "        s_theta -= theta_pert \n",
    "        M_vir_pert = hdf[f'system_{i}']['perturber_data']['M_vir'][()]\n",
    "        return s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s,s_s_true, s_s_error,s_v_los,s_v_los_true, s_v_los_error, s_theta, D, v_pert, M_vir_pert\n",
    "s0_xs, s0_ys, s0_zs, s0_vxs, s0_vys, s0_vzs, s0_s,s0_s_true, s0_s_error, s0_v_los,s0_v_los_true,s0_v_los_error, s0_theta, D0,v_pert,M_vir_pert = read_h5py_file(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_system(i):\n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s,s_s_true, s_s_error,s_v_los,s_v_los_true, s_v_los_error, s_theta, D, v_pert,M_vir_pert = read_h5py_file(i)\n",
    "    plt.plot(s_xs,s_ys,\".\")\n",
    "    plt.xlabel(\"X Position [Mpc]\")\n",
    "    plt.ylabel(\"Y Position [Mpc]\")\n",
    "    #plt.title(f\"System {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_system(5)\n",
    "#plt.savefig(\"system1_x_y_presentation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit analytical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def unprojected_v_model(s_primed,theta,t,D,mp,Omega_lambda):\n",
    "    r_primed = np.sqrt(D**2 + s_primed**2 - 2*D*s_primed*np.cos(theta))\n",
    "    term_1 = (r_primed/t)\n",
    "    term_2 = np.sqrt((const.G.value*mp)/r_primed)\n",
    "    argument = s_primed * np.sin(theta) / r_primed\n",
    "    argument = np.clip(argument, -1.0, 1.0) #Is this ok??\n",
    "    theta_primed = np.arcsin(argument)\n",
    "    expanssion_term = 0.31*Omega_lambda*r_primed/s_primed\n",
    "    mask = m_to_Mpc(s_primed*np.cos(theta)) < D\n",
    "    theta_primed[mask] = np.pi - theta_primed[mask]\n",
    "    term_3 = np.cos(theta_primed-theta)\n",
    "    return (1.2*term_1 - 1.1*term_2+expanssion_term)\n",
    "\"\"\"\n",
    "def unprojected_v_model(r_primed,t,mp,Omega_lambda):\n",
    "    return 1.2 * r_primed/t - 1.1 * np.sqrt(const.G.value*mp/r_primed) + 0.31 * Omega_lambda * r_primed/t\n",
    "\n",
    "def v_model_corr(s_primed,theta,t,D,mp,Omega_lambda,v_pert):\n",
    "    r_primed = np.sqrt(D**2 + s_primed**2 - 2*D*s_primed*np.cos(theta))\n",
    "    term_1 = (r_primed/t)\n",
    "    term_2 = np.sqrt((const.G.value*mp)/r_primed)\n",
    "    theta_primed = np.arcsin(s_primed * np.sin(theta) / r_primed)\n",
    "    expanssion_term = 0.31*Omega_lambda*r_primed/t\n",
    "    mask = s_primed*np.cos(theta) < D\n",
    "    theta_primed[mask] = np.pi - theta_primed[mask]\n",
    "    term_3 = np.cos(theta_primed-theta)\n",
    "    #print(term_1,term_2,term_3)\n",
    "    return (1.2*term_1 - 1.1*term_2+expanssion_term)*term_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_vir_calc(M_vir,h):\n",
    "    return ((M_vir * 75**3 * h ** (-2) / (1e11)) ** (1/3)) / 1000 # Convert to Mpc\n",
    "\n",
    "def plot_Lyndell(i):\n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s, s_s_true, s_s_error, s_v_los, s_v_los_true, s_v_los_error, s_theta, D, v_pert, M_vir_pert = read_h5py_file(i)\n",
    "    r_data = np.sqrt(s_xs**2 + s_ys**2 + s_zs**2)\n",
    "    t = compute_t(0.6911, 67.74)\n",
    "    v_model_6774 = unprojected_v_model(Mpc_to_m(r_data), Gyr_to_s(t), (M_vir_pert)*const.M_sun.value, 0.6911)/1000 # arreglar\n",
    "\n",
    "    t_50 = compute_t(0.6911, 50)\n",
    "    v_model_50 = unprojected_v_model(Mpc_to_m(r_data), Gyr_to_s(t_50), (M_vir_pert)*const.M_sun.value, 0.6911)/1000 # arreglar\n",
    "\n",
    "    t_40 = compute_t(0.6911, 40)\n",
    "    v_model_40 = unprojected_v_model(Mpc_to_m(r_data), Gyr_to_s(t_40), (M_vir_pert)*const.M_sun.value, 0.6911)/1000 # arreglar\n",
    "\n",
    "    t_80 = compute_t(0.6911, 80)\n",
    "    v_model_80 = unprojected_v_model(Mpc_to_m(r_data), Gyr_to_s(t_80), (M_vir_pert)*const.M_sun.value, 0.6911)/1000 # arreglar\n",
    "\n",
    "    v_data = (s_vxs*s_xs + s_vys*s_ys + s_vzs*s_zs)/r_data\n",
    "    r_vir = r_vir_calc(M_vir_pert, 0.6911)\n",
    "    \n",
    "    # Create figure and axes objects explicitly\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot on the axes object\n",
    "    ax.axvline(x=r_vir * 2, color='r', linestyle='--', label='2 Virial Radii')\n",
    "    ax.plot(r_data, v_data, \".\", label=\"Unscattered Uchuu Data\")\n",
    "    ax.plot(r_data, v_model_80, \".\", label=r\"Model, $M = M_{{vir}}, H_0 = 80 kms^{-1}Mpc^{-1}, \\Omega_{\\Lambda} = 0.6911$\")\n",
    "    ax.plot(r_data, v_model_6774, \".\", label=r\"Model, $M = M_{{vir}}, H_0 = 67.74 kms^{-1}Mpc^{-1}, \\Omega_{\\Lambda} = 0.6911$\")\n",
    "    ax.plot(r_data, v_model_50, \".\", label=r\"Model, $M = M_{{vir}}, H_0 = 50 kms^{-1}Mpc^{-1}, \\Omega_{\\Lambda} = 0.6911$\")\n",
    "    ax.plot(r_data, v_model_40, \".\", label=r\"Model, $M = M_{{vir}}, H_0 = 40 kms^{-1}Mpc^{-1}, \\Omega_{\\Lambda} = 0.6911$\")\n",
    "\n",
    "    # Set labels\n",
    "    ax.set_xlabel(\"Distance to the Perturber [Mpc]\")\n",
    "    ax.set_ylabel(\"Radial Velocity [km/s]\")\n",
    "    \n",
    "    # Ensure the legend is displayed with proper positioning\n",
    "    ax.legend(loc='best', frameon=True, framealpha=0.9)\n",
    "    \n",
    "    # Tight layout to ensure everything fits\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "plot_Lyndell(5)\n",
    "#plt.savefig(\"Lyndell_Bell_0_noise.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_satellites(i):\n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s,s_s_true, s_s_error,s_v_los,s_v_los_true, s_v_los_error, s_theta, D, v_pert,M_vir_pert = read_h5py_file(i)\n",
    "    r_data = np.sqrt(s_xs**2 + s_ys**2 + s_zs**2)\n",
    "    r_vir = r_vir_calc(M_vir_pert,0.69)\n",
    "    mask = r_data > 2*r_vir\n",
    "    s_xs = s_xs[mask]\n",
    "    s_ys = s_ys[mask]\n",
    "    s_zs = s_zs[mask]\n",
    "    s_vxs = s_vxs[mask]\n",
    "    s_vys = s_vys[mask]\n",
    "    s_vzs = s_vzs[mask]\n",
    "\n",
    "    return s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs,M_vir_pert\n",
    "\n",
    "def project_z_axis(s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, D):\n",
    "    s_zs += D\n",
    "    s_r= np.sqrt(s_xs**2 + s_ys**2 + s_zs**2)\n",
    "    s_theta = np.arccos(s_zs/s_r)\n",
    "    v_los = (s_vxs*s_xs+s_vys*s_ys+s_vzs*s_zs)/s_r\n",
    "\n",
    "    return s_r, s_theta, v_los\n",
    "\n",
    "def randomize_data_full(s_r, v_los, s_r_error, v_los_error):\n",
    "    s_r_error = np.full_like(s_r, s_r_error)\n",
    "    v_los_error = np.full_like(v_los, v_los_error)\n",
    "    s_r = np.random.normal(loc=s_r, scale=s_r_error, size=len(s_r))\n",
    "    v_los = np.random.normal(loc=v_los, scale=v_los_error, size=len(v_los))\n",
    "    return s_r, v_los, s_r_error, v_los_error\n",
    "\n",
    "def plot_bow_tie_system(s_r, s_theta, v_los,M,D,H_0,i):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(s_r,v_los,\".\",label = \"Uchuu Data\")\n",
    "    t = compute_t(0.6911,H_0)\n",
    "    v_calc = v_model(s_r,s_theta,t,D,M* const.M_sun.value,0.69)\n",
    "    plt.plot(s_r,v_calc,\".\", label = \"Analytical Model\")\n",
    "    plt.xlabel(\"Distance to the Observer [Mpc]\")\n",
    "    plt.ylabel(\"Line of Sight Velocity [km/s]\")\n",
    "    #plt.title(f\"System {i}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs,M_vir_pert = clean_satellites(5)\n",
    "#s_r, s_theta, v_los = project_z_axis(s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, 100)\n",
    "#s_r, v_los,s_r_error, v_los_error = randomize_data_full(s_r, v_los, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bow_tie_system(s_r, s_theta, v_los,M_vir_pert,100,67.74,5)\n",
    "#plt.savefig(\"Bow_Tie_System1_presentation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bow_tie_observers(i,randomized):\n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s,s_s_true, s_s_error,s_v_los,s_v_los_true, s_v_los_error, s_theta, D, v_pert,M_vir_pert= read_h5py_file(i)\n",
    "    if randomized == True:\n",
    "        plt.plot(s_s, s_v_los, \".\", label=\"Data\")\n",
    "        v_los_calc = v_model_corr(Mpc_to_m(s_s),s_theta,Gyr_to_s(13.7), Mpc_to_m(D),(1e13)*const.M_sun.value, 0.7,v_pert)/1000\n",
    "        plt.plot(s_s,v_los_calc,\".\",label = \"Model, M = 1e13 Msun\")\n",
    "    else:\n",
    "        plt.plot(s_s_true, s_v_los_true, \".\", label=\"True Data\")\n",
    "        v_los_calc = v_model_corr(Mpc_to_m(s_s_true),s_theta,Gyr_to_s(13.7), Mpc_to_m(D),(1e13)*const.M_sun.value, 0.7,v_pert)/1000\n",
    "        plt.plot(s_s_true,v_los_calc,\".\",label = \"Model, M = 1e13 Msun\")\n",
    "\n",
    "    #v_los_calc_14 = v_model(Mpc_to_m(s0_s),s0_theta,Gyr_to_s(13.7), Mpc_to_m(D0),(1e15)*const.M_sun.value, 0.7)/1000\n",
    "    #plt.plot(s0_s,v_los_calc_14,\".\",label = \"Model, M = 1e15 Msun\")\n",
    "    plt.xlabel(\"Distance from the Observer [Mpc]\")\n",
    "    plt.ylabel(\"Line of Sight Velocity [km/s]\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"System {i}\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"System_0_mass_change.png\")\n",
    "    plt.show()\n",
    "\n",
    "#plot_bow_tie(0,True)\n",
    "plot_bow_tie_observers(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stacked systems\n",
    "def plot_individual_systems(num_systems):\n",
    "    image_paths = []\n",
    "    \n",
    "    for i in range(num_systems):\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))  # Smaller, standard-sized images\n",
    "        s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s, s_s_true, s_s_error, s_v_los, s_v_los_true, s_v_los_error, s_theta, D, v_pert = read_h5py_file(i)\n",
    "        sns.scatterplot(x=s_s_true, y=s_v_los_true, ax=ax, s=10, color='blue', edgecolor='black')\n",
    "        ax.scatter(D, v_pert, color='red', marker='x', s=100, label='Perturber')  # Mark perturber position at (D, v_pert)\n",
    "        ax.set_ylabel(\"v [km/s]\", fontsize=12)\n",
    "        ax.set_xlabel(\"d [Mpc]\", fontsize=12)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.title(f\"System {i}\")\n",
    "        ax.legend()\n",
    "        \n",
    "        img_path = f\"system_{i}.png\"\n",
    "        plt.savefig(img_path, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        image_paths.append(img_path)\n",
    "    \n",
    "    # Combine images vertically\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_height = sum(heights)\n",
    "    max_width = max(widths)\n",
    "    \n",
    "    combined_image = Image.new('RGB', (max_width, total_height))\n",
    "    y_offset = 0\n",
    "    for img in images:\n",
    "        combined_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.size[1]\n",
    "    \n",
    "    combined_image.save(\"combined_bowtie.png\")\n",
    "    \n",
    "    # Clean up individual images\n",
    "    for img_path in image_paths:\n",
    "        os.remove(img_path)\n",
    "    \n",
    "    print(\"Saved combined image as combined_bowtie.png\")\n",
    "\n",
    "plot_individual_systems(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_grid_systems(num_systems, rows=2, cols=4):\n",
    "    \"\"\"\n",
    "    Plot systems in a grid layout suitable for a paper or project report.\n",
    "    \n",
    "    Parameters:\n",
    "    num_systems (int): Number of systems to plot\n",
    "    rows (int): Number of rows in the grid\n",
    "    cols (int): Number of columns in the grid\n",
    "    \"\"\"\n",
    "    # Create a figure with subplots arranged in a grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))\n",
    "    axes = axes.flatten()  # Flatten to make indexing easier\n",
    "    \n",
    "    # Plot each system\n",
    "    for i in range(num_systems):\n",
    "        ax = axes[i]\n",
    "        s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s, s_s_true, s_s_error, s_v_los, s_v_los_true, s_v_los_error, s_theta, D, v_pert,M_vir_pert = read_h5py_file(i)\n",
    "        \n",
    "        # Plot data points\n",
    "        sns.scatterplot(x=s_s_true, y=s_v_los_true, ax=ax, s=10, color='blue', edgecolor='black')\n",
    "        \n",
    "        # Mark perturber position\n",
    "        ax.scatter(D, v_pert, color='red', marker='x', s=100, label='Perturber')\n",
    "        \n",
    "        # Set labels and title\n",
    "        ax.set_ylabel(f\"${{v_{{los}}}}$ [km/s]\", fontsize=10)\n",
    "        ax.set_xlabel(\"d [Mpc]\", fontsize=10)\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        ax.set_title(f\"System {i}\", fontsize=11)\n",
    "        \n",
    "        # Add legend only if there's enough space\n",
    "        if i == 0:\n",
    "            ax.legend(fontsize=8)\n",
    "    \n",
    "    # Remove any unused subplots\n",
    "    for i in range(num_systems, rows*cols):\n",
    "        fig.delaxes(axes[i])\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.4)\n",
    "    \n",
    "    # Save the figure in high resolution\n",
    "    #plt.savefig(\"grid_systems.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(\"grid_systems.pdf\", format='pdf', bbox_inches='tight')  # PDF for publication quality\n",
    "    \n",
    "    plt.close(fig)\n",
    "    print(\"Saved grid image as grid_systems.pdf\")\n",
    "\n",
    "# Example usage\n",
    "plot_grid_systems(8)  # Default: 2 rows, 4 columns\n",
    "\n",
    "# If you prefer a different layout, you can specify rows and columns:\n",
    "# plot_grid_systems(8, rows=4, cols=2)  # 4 rows, 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_corr(phi, s, s_error, theta, v_los, v_los_error,D):\n",
    "    t = Gyr_to_s(phi[0])\n",
    "    mp = (10**phi[1])*const.M_sun.value\n",
    "    Omega_lambda = phi[2]\n",
    "    hyper = phi[3]\n",
    "    \n",
    "    num_samples = 1000 \n",
    "    total_log_likelihood = 0.0\n",
    "    for i in range(len(s)):\n",
    "        s_primed_samples = np.random.normal(s[i], s_error, num_samples)\n",
    "        v_los_samples = v_model(s_primed_samples, theta[i], t,D, mp, Omega_lambda)/1000\n",
    "        # Calculate the probability of obtaining v_los_samples from a normal distribution with mean v_los[i] and standard deviation v_los_error[i]\n",
    "        log_probs = norm.logpdf(\n",
    "            v_los_samples, loc=v_los[i], scale=math.sqrt(v_los_error**2+hyper**2)\n",
    "        )\n",
    "        log_likelihood_i = logsumexp(log_probs) - np.log(num_samples)\n",
    "        #print(log_likelihood_i)\n",
    "        total_log_likelihood += log_likelihood_i\n",
    "    return total_log_likelihood\n",
    "\n",
    "\n",
    "def log_prob_corr(phi, s, s_error,theta, v_los, v_los_error,D):\n",
    "    # Compute the log-prior\n",
    "    log_prior_value = log_prior(phi)\n",
    "    if not np.isfinite(log_prior_value):\n",
    "        print(f\"log_prior returned {log_prior_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    # Compute the log-likelihood\n",
    "    log_likelihood_value = log_likelihood_corr(phi, s, s_error, theta, v_los, v_los_error,D)\n",
    "    if not np.isfinite(log_likelihood_value):\n",
    "        print(f\"log_likelihood returned {log_likelihood_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    return log_prior_value + log_likelihood_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s0_v_los_error = np.full_like(s0_v_los, 20)\n",
    "\"\"\"\n",
    "def run_mcmc(s,s_error,v_los,v_los_error,theta,initial_phi,D,ndim,nwalkers,nsteps,nburn):\n",
    "    # Initial positions of walkers around the initial guess\n",
    "    p0 = initial_phi + 1e-2 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "    # Initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob_corr, args=(s, s_error,theta, v_los, v_los_error,D))\n",
    "\n",
    "    # Run burn-in phase to allow walkers to explore the parameter space\n",
    "    print(\"Running burn-in...\")\n",
    "    state = sampler.run_mcmc(p0, nburn, progress=True)\n",
    "    sampler.reset()\n",
    "\n",
    "    # Run the main MCMC sampling\n",
    "    print(\"Running MCMC sampling...\")\n",
    "    sampler.run_mcmc(state, nsteps, progress=True)\n",
    "\n",
    "    # Flatten the chain to get a 2D array of samples\n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    return sampler, samples\n",
    "\"\"\"\n",
    "\n",
    "def project_and_run_system_i(i,ndim,nwalkers,nsteps,nburn,D,s_error,v_los_error):\n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs,M_vir_pert= clean_satellites(i)\n",
    "    s_r, s_theta, v_los = project_z_axis(s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, D)\n",
    "    s_r, v_los,s_r_error, v_los_error = randomize_data_full(s_r, v_los, s_error, v_los_error)\n",
    "    initial_phi = [68, np.log10(M/const.M_sun.value), 0.69,50]\n",
    "    sampler, samples = run_mcmc(s_r,s_r_error,v_los,v_los_error,s_theta,initial_phi,D,ndim,nwalkers,nsteps,nburn)\n",
    "    return sampler, samples\n",
    "    \n",
    "def run_system_i(i,ndim,nwalkers,nsteps,nburn):\n",
    "    # Initial guesses for log parameters (log10 values of t and mp, and Omega_lambda)\n",
    "    initial_phi = [68, np.log10(M/const.M_sun.value), 0.69,50]  \n",
    "    s_xs, s_ys, s_zs, s_vxs, s_vys, s_vzs, s_s,s_s_true, s_s_error,s_v_los,s_v_los_true, s_v_los_error, s_theta, D, v_pert,M_vir_pert = read_h5py_file(i)\n",
    "    sampler, samples = run_mcmc(s_s,s_s_error,s_v_los,s_v_los_error,s_theta,initial_phi,D,ndim,nwalkers,nsteps,nburn)\n",
    "    return sampler, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 4             \n",
    "nwalkers = 16         \n",
    "nsteps = 5000 #5000\n",
    "nburn = 500 #500\n",
    "sampler4_no_err, samples4_no_err = project_and_run_system_i(4,ndim,nwalkers,nsteps,nburn,100,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_diagnostic_plots(sampler4_no_err, nwalkers,\"corner_system_4_cold_no_err\",M_vir_pert) #change fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean acceptance fraction:\", np.mean(sampler0.acceptance_fraction))\n",
    "try:\n",
    "    tau = sampler0.get_autocorr_time()\n",
    "    print(\"Autocorrelation time:\", tau)\n",
    "except Exception as e:\n",
    "    print(\"Could not compute autocorrelation time:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = sampler0.get_chain(flat=False)\n",
    "\n",
    "\n",
    "nsteps, nwalkers, ndim = samples0.shape\n",
    "\n",
    "# Define labels for your parameters (adjust as needed)\n",
    "parameter_labels = [\"t\", \"log(mp)\", \"Omega_lambda\",\"hyper\"]\n",
    "\n",
    "# Create a figure for each parameter\n",
    "for i in range(ndim):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for j in range(nwalkers):\n",
    "        plt.plot(samples0[:, j, i], alpha=0.5)\n",
    "    plt.title(f\"Trace Plot for Parameter '{parameter_labels[i]}'\")\n",
    "    plt.xlabel(\"Step Number\")\n",
    "    plt.ylabel(f\"Value of '{parameter_labels[i]}'\")\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(f\"trace_plot_s0_{parameter_labels[i]}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the log probability as a function of the step number\n",
    "log_prob_values0 = sampler0.get_log_prob()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(nwalkers):\n",
    "    plt.plot(log_prob_values0[:, i], alpha=0.5)\n",
    "plt.xlabel(\"Step Number\")\n",
    "plt.ylabel(\"Log Probability\")\n",
    "plt.title(\"Log Probability as a Function of Step Number\")\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"log_probabilities_s0.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M0 = 2.136108650723353906e+13\n",
    "samples0 = sampler0.get_chain(flat=True)\n",
    "fig = corner.corner(samples0, labels=[\"t\", \"log(mp)\", \"Omega_lambda\", \"hyper\"],\n",
    "                    truths=[13.7, np.log10(M0/const.M_sun.value), 0.7, 50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples0 = sampler0.get_chain(flat=False)\n",
    "log_prob_values0 = sampler0.get_log_prob()\n",
    "max_log_prob_idx = np.unravel_index(np.argmax(log_prob_values0), log_prob_values0.shape)\n",
    "best_params0 = samples0[max_log_prob_idx]\n",
    "print(\"Best-fitting parameters for sample 0:\", best_params) #not very good...\n",
    "print(\"Best log-probability for sample 0:\", log_prob_values0[max_log_prob_idx])\n",
    "#Best-fitting parameters for sample 0: [ 14.81986454  10.67615      0.53776146 173.41276779]\n",
    "log_prob_values1 = sampler1.get_log_prob()\n",
    "max_log_prob_idx1 = np.unravel_index(np.argmax(log_prob_values1), log_prob_values1.shape)\n",
    "best_params1 = samples1[max_log_prob_idx]\n",
    "print(\"Best-fitting parameters for sample 1:\", best_params1) #not very good...\n",
    "print(\"Best log-probability for sample 1:\", log_prob_values1[max_log_prob_idx1])\n",
    "m1 = (10**best_params1[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(best_params,s_data,theta_data,v_los_data,D):\n",
    "    v_calc = v_model(Mpc_to_m(s_data),theta_data,Gyr_to_s(13.7), Mpc_to_m(D), \n",
    "                      (3e16)*const.M_sun.value, 0.7)/1000\n",
    "    #v_calc = v_model(Mpc_to_m(s_data),theta_data,Gyr_to_s(best_params[0]), Mpc_to_m(D),\n",
    "                      #(best_params[1])*const.M_sun.value, best_params[2])/1000\n",
    "    # Plot the bow tie for both the true and the model data\n",
    "    plt.plot(s_data, v_los_data, \".\", label=\"True Data\")\n",
    "    plt.plot(s_data, v_calc, \".\", label=\"MCMC params\")\n",
    "    plt.xlabel(\"Distance from Earth [Mpc]\")\n",
    "    plt.ylabel(\"Line of Sight Velocity [km/s]\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Model Data vs True Data\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"mcmc_vs_true_s0.png\")\n",
    "    plt.show()\n",
    "test_params(best_params0,s0_s,s0_theta,s0_v_los,D0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrain H_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define precomputed range for Omega_Lambda\n",
    "Omega_Lambda_vals = np.linspace(0.01, 0.99, 100)  # Avoiding extreme values\n",
    "\n",
    "# Function for integral\n",
    "def integrand(a, Omega_Lambda):\n",
    "    Omega_m = 1 - Omega_Lambda # Check with Jorge\n",
    "    return 1 / (a * np.sqrt(Omega_m * a**-3 + Omega_Lambda))\n",
    "\n",
    "# Compute integral for each Omega_Lambda\n",
    "integral_vals = np.array([\n",
    "    quad(integrand, 1e-5, 1, args=(Omega_Lambda))[0] for Omega_Lambda in Omega_Lambda_vals\n",
    "])\n",
    "\n",
    "# Create an interpolation function\n",
    "integral_interp = interp1d(Omega_Lambda_vals, integral_vals, kind='cubic', fill_value=\"extrapolate\")\n",
    "\n",
    "# Function to get H_0 using interpolation\n",
    "def compute_t(Omega_Lambda,H_0):\n",
    "    integral_result = integral_interp(Omega_Lambda)\n",
    "    t = integral_result / m_to_Mpc(H_0*1000)\n",
    "    return s_to_Gyr(t) # Convert to gyr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mpc_to_m(integral_interp(0.69)/Gyr_to_s(13.7)/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_t(0.69,65.13796434)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mpc_to_m(HUBBLE_CONST/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corner_Hubble(sampler, fname, M_True):\n",
    "    # Use a moderate font size\n",
    "    plt.rcParams.update({'font.size': 11})\n",
    "    \n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    \n",
    "    # Create the corner plot with standard settings\n",
    "    fig = corner.corner(\n",
    "        samples, \n",
    "        labels=[r\"${H_0}$ [km/s/Mpc]\", r\"$\\log_{10}(M)$\", r\"$\\Omega_\\Lambda$\", r\"$\\sigma_{\\rm hyper}$ [km/s]\"],\n",
    "        truths=[Mpc_to_m(HUBBLE_CONST/1000), np.log10(M_True/const.M_sun.value), 0.69, 50],\n",
    "        show_titles=True,\n",
    "        title_kwargs={\"fontsize\": 11}\n",
    "    )\n",
    "    \n",
    "    # Get the axes\n",
    "    axes = np.array(fig.axes).reshape((4, 4))\n",
    "    \n",
    "    # Customize tick formatters based on the parameter ranges\n",
    "    formatters = [\n",
    "        plt.FormatStrFormatter('%.1f'),       # For H0\n",
    "        plt.FormatStrFormatter('%.1f'),       # For log10(M)\n",
    "        plt.FormatStrFormatter('%.2f'),       # For Omega_Lambda\n",
    "        plt.FormatStrFormatter('%.0f')        # For sigma_hyper\n",
    "    ]\n",
    "    \n",
    "    # Apply formatters to bottom row axes and adjust spacing\n",
    "    for i in range(4):\n",
    "        for j in range(i+1):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Format x-axis ticks appropriately - use the proper formatter for this column\n",
    "            ax.xaxis.set_major_formatter(formatters[j])\n",
    "            \n",
    "            # Adjust number of ticks - this keeps layout compact but readable\n",
    "            ax.xaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "            \n",
    "            # Only for bottom row, add more space for labels\n",
    "            if i == 3:\n",
    "                ax.set_xlabel(ax.get_xlabel(), labelpad=10)\n",
    "    \n",
    "    # Make the overall figure slightly taller to accommodate labels\n",
    "    fig_width, fig_height = fig.get_size_inches()\n",
    "    fig.set_size_inches(fig_width, fig_height * 1.1)\n",
    "    \n",
    "    # Add a bit more space at the bottom\n",
    "    plt.subplots_adjust(bottom=0.12)\n",
    "    \n",
    "    fig.savefig(fname, dpi=200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "def plot_traces_Hubble(sampler):\n",
    "    samples = sampler.get_chain(flat=False)\n",
    "    # Retrieve the number of walkers and dimensions from the samples array\n",
    "    nsteps, nwalkers, ndim = samples.shape\n",
    "\n",
    "    # Define labels for your parameters (adjust as needed)\n",
    "    parameter_labels = [r\"${H_0}$\", r\"$log_{10}(M)$\", r\"$\\Omega_\\Lambda$\",r\"$\\sigma_{hyper}$\"]\n",
    "\n",
    "    # Create a figure for each parameter\n",
    "    for i in range(ndim):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for j in range(nwalkers):\n",
    "            plt.plot(samples[:, j, i], alpha=0.5)\n",
    "        plt.title(f\"Trace Plot for Parameter '{parameter_labels[i]}'\")\n",
    "        plt.xlabel(\"Step Number\")\n",
    "        plt.ylabel(f\"Value of '{parameter_labels[i]}'\")\n",
    "        plt.grid(True)\n",
    "        #plt.savefig(f\"new_trace_plot_{parameter_labels[i]}.png\")\n",
    "        plt.show()\n",
    "def find_best_params(sampler):\n",
    "    samples = sampler.get_chain(flat=True)  # Flatten the chain\n",
    "    log_prob_values = sampler.get_log_prob(flat=True)  # Flatten log-prob\n",
    "\n",
    "    # Find the best-fit parameters (corresponding to max log-probability)\n",
    "    max_log_prob_idx = np.argmax(log_prob_values)\n",
    "    best_params = samples[max_log_prob_idx]\n",
    "\n",
    "    # Compute mean and standard deviation for errors\n",
    "    means = np.mean(samples, axis=0)\n",
    "    std_devs = np.std(samples, axis=0)\n",
    "\n",
    "    print(\"Best-fitting parameters: \", best_params)\n",
    "    print(\"Mean parameter values:    \", means)\n",
    "    print(\"Parameter uncertainties (1σ): \", std_devs)\n",
    "\n",
    "    return best_params, means, std_devs  \n",
    "\n",
    "def run_diagnostic_plots_Hubble(sampler,nwalkers,fname,M_True):\n",
    "    plot_corner_Hubble(sampler,fname,M_True)\n",
    "    plot_traces_Hubble(sampler)\n",
    "    acceptance_fraction_calc(sampler)\n",
    "    plot_log_probabilities(sampler,nwalkers)\n",
    "    best_params, means, std_devs = find_best_params(sampler)\n",
    "    return best_params, means, std_devs\n",
    "#run_diagnostic_plots(sampler_TEST4,nwalkers,\"corner_plot_TEST4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "    \n",
    "def log_prior_Hubble(phi):\n",
    "    H_0, mp, Omega_lambda,hyper = phi\n",
    "    if not (10 < mp < 16 and hyper > 0 and 0<H_0<100):\n",
    "        return -np.inf\n",
    "\n",
    "    #log_prior_H0 = norm.logpdf(H_0, loc=67, scale=0.04)\n",
    "    #log_prior_mp = norm.logpdf(mp, loc=13, scale=2)\n",
    "    log_prior_Omega = norm.logpdf(Omega_lambda, loc=0.685, scale=0.007)\n",
    "\n",
    "    #return log_prior_H0 + log_prior_mp + log_prior_Omega\n",
    "    return log_prior_Omega \n",
    "    \n",
    "def log_likelihood_Hubble(phi, s, s_error, theta, v_los, v_los_error,D):\n",
    "    H_0 = phi[0]\n",
    "    mp = (10**phi[1])*const.M_sun.value\n",
    "    Omega_lambda = phi[2]\n",
    "    hyper = phi[3]\n",
    "    num_samples = 1000 \n",
    "    total_log_likelihood = 0.0\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        s_primed_samples = np.random.normal(s[i], s_error[i], num_samples)\n",
    "        t = compute_t(Omega_lambda,H_0)\n",
    "        v_los_samples = v_model(s_primed_samples, theta[i], t,D, mp, Omega_lambda) #Check the value of D\n",
    "        # Calculate the probability of obtaining v_los_samples from a normal distribution with mean v_los[i] and standard deviation v_los_error[i]\n",
    "        log_probs = norm.logpdf(\n",
    "            v_los_samples, loc=v_los[i], scale=math.sqrt(v_los_error[i]**2+hyper**2)\n",
    "        )\n",
    "\n",
    "        log_likelihood_i = logsumexp(log_probs) - np.log(num_samples)\n",
    "        #print(log_likelihood_i)\n",
    "        total_log_likelihood += log_likelihood_i\n",
    "    return total_log_likelihood\n",
    "\n",
    "\n",
    "def log_prob_Hubble(phi, s, s_error,theta, v_los, v_los_error,D):\n",
    "    # Compute the log-prior\n",
    "    log_prior_value = log_prior_Hubble(phi)\n",
    "    if not np.isfinite(log_prior_value):\n",
    "        #print(f\"log_prior returned {log_prior_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    # Compute the log-likelihood\n",
    "    log_likelihood_value = log_likelihood_Hubble(phi, s, s_error, theta, v_los, v_los_error,D)\n",
    "    if not np.isfinite(log_likelihood_value):\n",
    "        #print(f\"log_likelihood returned {log_likelihood_value} for phi = {phi}\")\n",
    "        return -np.inf\n",
    "\n",
    "    return log_prior_value + log_likelihood_value\n",
    "\n",
    "def run_mcmc(s,s_error,v_los,v_los_error,theta,initial_phi,D,ndim,nwalkers,nsteps,nburn):\n",
    "\n",
    "    # Initial positions of walkers around the initial guess\n",
    "    p0 = initial_phi + 1e-2 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "    # Initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_prob_Hubble, args=(s, s_error,theta, v_los, v_los_error,D))\n",
    "\n",
    "    # Run burn-in phase to allow walkers to explore the parameter space\n",
    "    print(\"Running burn-in...\")\n",
    "    state = sampler.run_mcmc(p0, nburn, progress=True)\n",
    "    sampler.reset()\n",
    "\n",
    "    # Run the main MCMC sampling\n",
    "    print(\"Running MCMC sampling...\")\n",
    "    sampler.run_mcmc(state, nsteps, progress=True)\n",
    "\n",
    "    # Flatten the chain to get a 2D array of samples\n",
    "    samples = sampler.get_chain(flat=True)\n",
    "    return sampler, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner  \n",
    "import math\n",
    "\n",
    "s = mock_data_df[\"s\"]          # Array of observed `s` values\n",
    "s_error = mock_data_df[\"s_error\"]       # Array of errors associated with `s`\n",
    "v_los = mock_data_df[\"v_los\"]       # Array of observed line-of-sight velocities\n",
    "v_los_error = mock_data_df[\"v_los_error\"] \n",
    "theta = mock_data_df[\"theta\"]    # Array of errors associated with `v_los`\n",
    "\n",
    "# Initial guesses for log parameters (log10 values of t and mp, and Omega_lambda)\n",
    "initial_phi = [(68), np.log10(M/const.M_sun.value), 0.69,50]  \n",
    "# MCMC parameters\n",
    "ndim = 4             # Number of parameters in `phi`\n",
    "nwalkers = 16         # Reduced number of walkers\n",
    "nsteps = 3500 # 50 x autocorrelation time 300 x 3500\n",
    "nburn = 300\n",
    "D = 100\n",
    "sampler_Hubble_test, samples_Hubble_test = run_mcmc(s,s_error,v_los,v_los_error,theta,initial_phi,D,ndim,nwalkers,nsteps,nburn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, means, std_devs = run_diagnostic_plots_Hubble(sampler_Hubble_test,nwalkers,\"corner_plot_Hubble_Plank_2.png\",M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vary the error in distance as a function of the turnaround radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_snapshot = pd.read_csv(\"final_snapshot_units.csv\")\n",
    "#  find the turnaround radius\n",
    "def find_turnaround(r_values,v_values):\n",
    "    # Sort data by r\n",
    "    sorted_indices = np.argsort(r_values)\n",
    "    r_sorted = r_values[sorted_indices]\n",
    "    v_sorted = v_values[sorted_indices]\n",
    "\n",
    "    # Find where v crosses zero\n",
    "    for i in range(len(v_sorted) - 1):\n",
    "        if np.sign(v_sorted[i]) != np.sign(v_sorted[i + 1]):  # Sign change detected\n",
    "            r1, r2 = r_sorted[i], r_sorted[i + 1]\n",
    "            v1, v2 = v_sorted[i], v_sorted[i + 1]\n",
    "            \n",
    "            # Linear interpolation for r when v = 0\n",
    "            r_zero = r1 + (0 - v1) * (r2 - r1) / (v2 - v1)\n",
    "            break  # Stop since we know there's only one crossing\n",
    "    return r_zero\n",
    "\n",
    "r_turnaround = find_turnaround(df_final_snapshot[\"r_primed\"],df_final_snapshot[\"velocity\"])\n",
    "print(f\" Turnaround radius: {r_turnaround} Mpc\")\n",
    "\n",
    "def run_varying_s_error(v_error,hyper):\n",
    "    percentage_errors = [0.01,0.05,0.10,0.25,0.5]\n",
    "    best_params_calc = []\n",
    "    means_calc = []\n",
    "    stdves_calc = []\n",
    "    for percentage in tqdm(percentage_errors):\n",
    "        s_error = r_turnaround * percentage\n",
    "        s = df_final_snapshot[\"distances\"]\n",
    "        v_los = df_final_snapshot[\"v_los\"]\n",
    "        theta = df_final_snapshot[\"theta\"]\n",
    "        #randomize the data using randomize_data()\n",
    "        s, v_los, s_error, v_los_error = randomize_data_full(s, v_los, s_error, np.sqrt(v_error**2 + hyper**2))\n",
    "        v_los_error = np.full_like(v_los, v_error)\n",
    "        initial_phi = [(68), np.log10(M/const.M_sun.value), 0.69,hyper]\n",
    "        D = 100\n",
    "        ndim = 4\n",
    "        nwalkers = 16\n",
    "        nsteps = 3500 # 3500\n",
    "        nburn = 300 # 300\n",
    "        sampler, samples = run_mcmc(s,s_error,v_los,v_los_error,theta,initial_phi,D,ndim,nwalkers,nsteps,nburn)\n",
    "        best_params, means, std_devs = run_diagnostic_plots_Hubble(sampler,nwalkers,f\"corner_plot_{percentage}.png\",M)\n",
    "        means_calc.append(means[0])\n",
    "        stdves_calc.append(std_devs[0])\n",
    "        best_params_calc.append(best_params[0])\n",
    "    return percentage_errors, best_params_calc, means_calc, stdves_calc\n",
    "def run_varying_v_error(hyper):\n",
    "    v_error_values = [0,50,100,150,200]  # Example absolute values\n",
    "    best_params_calc = []\n",
    "    means_calc = []\n",
    "    stdves_calc = []\n",
    "    \n",
    "    for v_error in tqdm(v_error_values):\n",
    "        s = df_final_snapshot[\"distances\"]\n",
    "        v_los = df_final_snapshot[\"v_los\"]\n",
    "        theta = df_final_snapshot[\"theta\"]\n",
    "        s_error = 0.1 # Fixed error for s_error\n",
    "        \n",
    "        # Randomize the data\n",
    "        s, v_los, s_error, v_los_error = randomize_data_full(s, v_los, s_error, np.sqrt(v_error**2 + hyper**2))\n",
    "        v_los_error = np.full_like(v_los, v_error)\n",
    "        \n",
    "        initial_phi = [(68), np.log10(M/const.M_sun.value), 0.69, hyper]\n",
    "        D = 100\n",
    "        ndim = 4\n",
    "        nwalkers = 16\n",
    "        nsteps = 3500\n",
    "        nburn = 300\n",
    "        \n",
    "        sampler, samples = run_mcmc(s, s_error, v_los, v_los_error, theta, initial_phi, D, ndim, nwalkers, nsteps, nburn)\n",
    "        best_params, means, std_devs = run_diagnostic_plots_Hubble(sampler, nwalkers, f\"corner_plot_v{v_error}.png\", M)\n",
    "        \n",
    "        means_calc.append(means[0])\n",
    "        stdves_calc.append(std_devs[0])\n",
    "        best_params_calc.append(best_params[0])\n",
    "    \n",
    "    return v_error_values, best_params_calc, means_calc, stdves_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage_errors, best_params_calc, means_calc, stdves_calc = run_varying_s_error(20,50)\n",
    "percentage_errors_v, best_params_calc_v, means_calc_v, stdves_calc_v = run_varying_v_error(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(percentage_errors, means_calc, yerr=stdves_calc, fmt='o', color='black')\n",
    "# add a horizontal line for the true value of H_0   \n",
    "plt.axhline(y=HUBBLE_CONST*(const.pc.value*1e3), color='red', linestyle='--', label='True Value')\n",
    "plt.xlabel(\"Error in turnaroud radius units (1.44 Mpc)\")\n",
    "plt.ylabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "plt.legend()\n",
    "plt.savefig(\"s_errors.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save percentage_errors, best_params_calc, means_calc, stdves_calc in a text file\n",
    "data_to_save = {\n",
    "    \"percentage_errors\": percentage_errors,\n",
    "    \"best_params_calc\": best_params_calc,\n",
    "    \"means_calc\": means_calc,\n",
    "    \"stdves_calc\": stdves_calc\n",
    "}\n",
    "\n",
    "with open('mcmc_results_s_errors.txt', 'w') as file:\n",
    "    for key, value in data_to_save.items():\n",
    "        file.write(f\"{key}: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
